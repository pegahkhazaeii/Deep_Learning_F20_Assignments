{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":23,"outputs":[{"output_type":"stream","text":"/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv\n/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\n/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport unicodedata, re, string\nimport nltk\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport seaborn as sns\nsns.set(color_codes=True)\n\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))","execution_count":22,"outputs":[{"output_type":"stream","text":"['sentiment-analysis-on-movie-reviews']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/sentiment-analysis-on-movie-reviews\"))","execution_count":26,"outputs":[{"output_type":"stream","text":"['sampleSubmission.csv', 'train.tsv.zip', 'test.tsv.zip']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/sentiment-analysis-on-movie-reviews/train.tsv.zip\", sep=\"\\t\")\ndf_test = pd.read_csv(\"../input/sentiment-analysis-on-movie-reviews/test.tsv.zip\", sep=\"\\t\")","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":28,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 156060 entries, 0 to 156059\nData columns (total 4 columns):\n #   Column      Non-Null Count   Dtype \n---  ------      --------------   ----- \n 0   PhraseId    156060 non-null  int64 \n 1   SentenceId  156060 non-null  int64 \n 2   Phrase      156060 non-null  object\n 3   Sentiment   156060 non-null  int64 \ndtypes: int64(3), object(1)\nmemory usage: 4.8+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"   PhraseId  SentenceId                                             Phrase  \\\n0         1           1  A series of escapades demonstrating the adage ...   \n1         2           1  A series of escapades demonstrating the adage ...   \n2         3           1                                           A series   \n3         4           1                                                  A   \n4         5           1                                             series   \n\n   Sentiment  \n0          1  \n1          2  \n2          2  \n3          2  \n4          2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>A series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>series</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_numbers(words):\n    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(\"\\d+\", \"\", word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = remove_numbers(words)\n\n    return words","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train['Words'] = df_train['Phrase'].apply(nltk.word_tokenize)\n\ndf_train['Words'] = df_train['Words'].apply(normalize) \ndf_train['Words'].head()","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"0    [a, series, of, escapades, demonstrating, the,...\n1    [a, series, of, escapades, demonstrating, the,...\n2                                          [a, series]\n3                                                  [a]\n4                                             [series]\nName: Words, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_set = set()\nfor l in df_train['Words']:\n    for e in l:\n        word_set.add(e)\n        \nword_to_int = {word: ii for ii, word in enumerate(word_set, 1)}\n\n\nprint(len(word_set))\nprint(len(word_to_int))","execution_count":33,"outputs":[{"output_type":"stream","text":"16209\n16209\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Tokens'] = df_train['Words'].apply(lambda l: [word_to_int[word] for word in l])\ndf_train['Tokens'].head()","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"0    [10431, 1999, 13520, 14478, 7758, 2054, 3873, ...\n1    [10431, 1999, 13520, 14478, 7758, 2054, 3873, ...\n2                                        [10431, 1999]\n3                                              [10431]\n4                                               [1999]\nName: Tokens, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = df_train['Tokens'].str.len().max()\nprint(max_len)","execution_count":35,"outputs":[{"output_type":"stream","text":"48\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_tokens = np.array([t for t in df_train['Tokens']])\nencoded_labels = np.array([l for l in df_train['Sentiment']])\n\n\nfeatures = np.zeros((len(all_tokens), max_len), dtype=int)\n\nfor i, row in enumerate(all_tokens):\n    features[i, :len(row)] = row\n\nprint(features[:3])\n","execution_count":36,"outputs":[{"output_type":"stream","text":"[[10431  1999 13520 14478  7758  2054  3873  9404  2031  2106 11674  8337\n   2054 12821  2106  9102 11674  8337  2054  4848  1934 13520 11139  2150\n  12567 10479  6619 13520 11139  5911 15296 15294 13520 10431  9618     0\n      0     0     0     0     0     0     0     0     0     0     0     0]\n [10431  1999 13520 14478  7758  2054  3873  9404  2031  2106 11674  8337\n   2054 12821     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0]\n [10431  1999     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_frac = 0.8\n\n## split data into training, validation, and test data (features and labels, x and y)\n\nsplit_idx = int(len(features)*0.8)\ntrain_x, remaining_x = features[:split_idx], features[split_idx:]\ntrain_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)*0.5)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n\n## print out the shapes of  resultant feature data\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n      \"\\nValidation set: \\t{}\".format(val_x.shape),\n      \"\\nTest set: \\t\\t{}\".format(test_x.shape))","execution_count":37,"outputs":[{"output_type":"stream","text":"\t\t\tFeature Shapes:\nTrain set: \t\t(124848, 48) \nValidation set: \t(15606, 48) \nTest set: \t\t(15606, 48)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\n# dataloaders\nbatch_size = 54\n\n# make sure the SHUFFLE your training data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\n# Check the size of the loaders (how many batches inside)\nprint(len(train_loader))\nprint(len(valid_loader))\nprint(len(test_loader))","execution_count":38,"outputs":[{"output_type":"stream","text":"2312\n289\n289\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","execution_count":39,"outputs":[{"output_type":"stream","text":"No GPU available, training on CPU.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentRNN(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentRNN, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear\n        self.fc = nn.Linear(hidden_dim, output_size)\n        \n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n\n        # embeddings and lstm_out\n        embeds = self.embedding(x)\n\n        lstm_out, hidden = self.lstm(embeds, hidden)\n\n        # transform lstm output to input size of linear layers\n        lstm_out = lstm_out.transpose(0,1)\n        lstm_out = lstm_out[-1]\n\n        out = self.dropout(lstm_out)\n        out = self.fc(out)        \n\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model w/ hyperparams\nvocab_size = len(word_to_int)+1 # +1 for the 0 padding\noutput_size = 5\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\n\nnet = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n\nprint(net)","execution_count":41,"outputs":[{"output_type":"stream","text":"SentimentRNN(\n  (embedding): Embedding(16210, 400)\n  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=5, bias=True)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss and optimization functions\nlr=0.003\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training params\nepochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n\n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n        # calculate the loss and perform backprop\n        loss = criterion(output, labels)\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output, labels)\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","execution_count":43,"outputs":[{"output_type":"stream","text":"Epoch: 1/3... Step: 100... Loss: 1.208542... Val Loss: 1.303120\nEpoch: 1/3... Step: 200... Loss: 1.443493... Val Loss: 1.304100\nEpoch: 1/3... Step: 300... Loss: 1.248297... Val Loss: 1.310628\nEpoch: 1/3... Step: 400... Loss: 1.400023... Val Loss: 1.302720\nEpoch: 1/3... Step: 500... Loss: 1.433426... Val Loss: 1.301408\nEpoch: 1/3... Step: 600... Loss: 1.278116... Val Loss: 1.302044\nEpoch: 1/3... Step: 700... Loss: 1.355534... Val Loss: 1.303510\nEpoch: 1/3... Step: 800... Loss: 1.208922... Val Loss: 1.305357\nEpoch: 1/3... Step: 900... Loss: 1.146682... Val Loss: 1.302960\nEpoch: 1/3... Step: 1000... Loss: 1.278372... Val Loss: 1.302660\nEpoch: 1/3... Step: 1100... Loss: 1.179437... Val Loss: 1.292583\nEpoch: 1/3... Step: 1200... Loss: 1.243635... Val Loss: 1.242504\nEpoch: 1/3... Step: 1300... Loss: 1.118566... Val Loss: 1.235344\nEpoch: 1/3... Step: 1400... Loss: 1.422111... Val Loss: 1.214825\nEpoch: 1/3... Step: 1500... Loss: 0.999055... Val Loss: 1.211832\nEpoch: 1/3... Step: 1600... Loss: 1.352970... Val Loss: 1.198710\nEpoch: 1/3... Step: 1700... Loss: 1.050932... Val Loss: 1.197443\nEpoch: 1/3... Step: 1800... Loss: 1.351266... Val Loss: 1.184685\nEpoch: 1/3... Step: 1900... Loss: 1.095386... Val Loss: 1.176798\nEpoch: 1/3... Step: 2000... Loss: 0.999879... Val Loss: 1.183058\nEpoch: 1/3... Step: 2100... Loss: 0.902429... Val Loss: 1.152630\nEpoch: 1/3... Step: 2200... Loss: 1.166427... Val Loss: 1.159060\nEpoch: 1/3... Step: 2300... Loss: 0.843685... Val Loss: 1.146438\nEpoch: 2/3... Step: 2400... Loss: 0.988838... Val Loss: 1.133045\nEpoch: 2/3... Step: 2500... Loss: 1.143136... Val Loss: 1.116046\nEpoch: 2/3... Step: 2600... Loss: 1.140034... Val Loss: 1.130166\nEpoch: 2/3... Step: 2700... Loss: 0.934852... Val Loss: 1.109873\nEpoch: 2/3... Step: 2800... Loss: 0.996788... Val Loss: 1.106920\nEpoch: 2/3... Step: 2900... Loss: 0.995063... Val Loss: 1.102939\nEpoch: 2/3... Step: 3000... Loss: 0.993651... Val Loss: 1.082549\nEpoch: 2/3... Step: 3100... Loss: 1.043449... Val Loss: 1.094692\nEpoch: 2/3... Step: 3200... Loss: 1.034948... Val Loss: 1.108344\nEpoch: 2/3... Step: 3300... Loss: 1.029526... Val Loss: 1.085963\nEpoch: 2/3... Step: 3400... Loss: 1.037004... Val Loss: 1.072551\nEpoch: 2/3... Step: 3500... Loss: 1.279297... Val Loss: 1.111236\nEpoch: 2/3... Step: 3600... Loss: 0.997148... Val Loss: 1.101529\nEpoch: 2/3... Step: 3700... Loss: 0.769597... Val Loss: 1.074170\nEpoch: 2/3... Step: 3800... Loss: 0.915522... Val Loss: 1.077430\nEpoch: 2/3... Step: 3900... Loss: 0.951793... Val Loss: 1.087201\nEpoch: 2/3... Step: 4000... Loss: 0.977325... Val Loss: 1.057959\nEpoch: 2/3... Step: 4100... Loss: 0.965014... Val Loss: 1.076207\nEpoch: 2/3... Step: 4200... Loss: 1.238408... Val Loss: 1.090293\nEpoch: 2/3... Step: 4300... Loss: 0.912587... Val Loss: 1.073071\nEpoch: 2/3... Step: 4400... Loss: 0.922302... Val Loss: 1.074532\nEpoch: 2/3... Step: 4500... Loss: 0.905242... Val Loss: 1.064656\nEpoch: 2/3... Step: 4600... Loss: 0.741885... Val Loss: 1.079205\nEpoch: 3/3... Step: 4700... Loss: 0.791580... Val Loss: 1.066876\nEpoch: 3/3... Step: 4800... Loss: 0.877480... Val Loss: 1.102561\nEpoch: 3/3... Step: 4900... Loss: 1.013675... Val Loss: 1.086293\nEpoch: 3/3... Step: 5000... Loss: 0.929336... Val Loss: 1.064131\nEpoch: 3/3... Step: 5100... Loss: 0.677867... Val Loss: 1.093358\nEpoch: 3/3... Step: 5200... Loss: 0.914148... Val Loss: 1.097790\nEpoch: 3/3... Step: 5300... Loss: 0.746694... Val Loss: 1.077821\nEpoch: 3/3... Step: 5400... Loss: 0.888740... Val Loss: 1.084576\nEpoch: 3/3... Step: 5500... Loss: 0.834917... Val Loss: 1.064383\nEpoch: 3/3... Step: 5600... Loss: 0.808635... Val Loss: 1.074279\nEpoch: 3/3... Step: 5700... Loss: 0.979789... Val Loss: 1.079970\nEpoch: 3/3... Step: 5800... Loss: 0.939775... Val Loss: 1.089641\nEpoch: 3/3... Step: 5900... Loss: 0.975640... Val Loss: 1.064761\nEpoch: 3/3... Step: 6000... Loss: 0.921004... Val Loss: 1.082868\nEpoch: 3/3... Step: 6100... Loss: 0.767276... Val Loss: 1.063706\nEpoch: 3/3... Step: 6200... Loss: 0.830540... Val Loss: 1.058390\nEpoch: 3/3... Step: 6300... Loss: 0.883353... Val Loss: 1.066880\nEpoch: 3/3... Step: 6400... Loss: 0.849517... Val Loss: 1.055286\nEpoch: 3/3... Step: 6500... Loss: 0.752413... Val Loss: 1.042469\nEpoch: 3/3... Step: 6600... Loss: 0.857376... Val Loss: 1.051557\nEpoch: 3/3... Step: 6700... Loss: 0.677751... Val Loss: 1.092948\nEpoch: 3/3... Step: 6800... Loss: 0.719640... Val Loss: 1.075300\nEpoch: 3/3... Step: 6900... Loss: 0.775304... Val Loss: 1.081310\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}