{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport random\nimport time\nimport string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\nfrom torchtext import data\nimport torch.optim as optim\nfrom torchtext.vocab import Vectors\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":22,"outputs":[{"output_type":"stream","text":"/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv\n/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\n/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip '/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip'\n!unzip '/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip'","execution_count":30,"outputs":[{"output_type":"stream","text":"Archive:  /kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\n  inflating: train.tsv               \nArchive:  /kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip\n  inflating: test.tsv                \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport random\nimport pickle\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom torch.utils.data import DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('train.tsv', sep='\\t')\ntest = pd.read_csv('test.tsv', sep='\\t')","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Corpus_Extr(df):\n    print('Construct Corpus...')\n    corpus = []\n    for i in tqdm(range(len(df))):\n        corpus.append(df.Phrase[i].lower().split())\n    corpus = Counter(np.hstack(corpus))\n    corpus = corpus\n    corpus2 = sorted(corpus,key=corpus.get,reverse=True)\n    print('Convert Corpus to Integers')\n    vocab_to_int = {word: idx for idx,word in enumerate(corpus2,1)}\n    print('Convert Phrase to Integers')\n    phrase_to_int = []\n    for i in tqdm(range(len(df))):\n        phrase_to_int.append([vocab_to_int[word] for word in df.Phrase.values[i].lower().split()])\n    return corpus,vocab_to_int,phrase_to_int\ncorpus,vocab_to_int,phrase_to_int = Corpus_Extr(train)","execution_count":34,"outputs":[{"output_type":"stream","text":"  3%|▎         | 4259/156060 [00:00<00:03, 42588.52it/s]","name":"stderr"},{"output_type":"stream","text":"Construct Corpus...\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 156060/156060 [00:03<00:00, 41909.87it/s]\n  5%|▌         | 8422/156060 [00:00<00:01, 82957.41it/s]","name":"stderr"},{"output_type":"stream","text":"Convert Corpus to Integers\nConvert Phrase to Integers\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 156060/156060 [00:02<00:00, 74229.74it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Pad_sequences(phrase_to_int,seq_length):\n    pad_sequences = np.zeros((len(phrase_to_int), seq_length),dtype=int)\n    for idx,row in tqdm(enumerate(phrase_to_int),total=len(phrase_to_int)):\n        pad_sequences[idx, :len(row)] = np.array(row)[:seq_length]\n    return pad_sequences","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_sequences = Pad_sequences(phrase_to_int,30)","execution_count":36,"outputs":[{"output_type":"stream","text":"100%|██████████| 156060/156060 [00:00<00:00, 163022.39it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(50)","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"        PhraseId  SentenceId  \\\n96584      96585        5045   \n87200      87201        4523   \n58251      58252        2935   \n6661        6662         265   \n15383      15384         658   \n92337      92338        4806   \n78631      78632        4047   \n125370    125371        6737   \n120529    120530        6445   \n43199      43200        2087   \n110286    110287        5840   \n33558      33559        1574   \n105339    105340        5562   \n119705    119706        6403   \n17400      17401         755   \n39025      39026        1862   \n95315      95316        4975   \n98463      98464        5161   \n112535    112536        5977   \n124336    124337        6678   \n142156    142157        7711   \n124574    124575        6693   \n83694      83695        4326   \n119917    119918        6413   \n143267    143268        7777   \n23761      23762        1080   \n148501    148502        8080   \n19786      19787         873   \n14995      14996         645   \n148165    148166        8062   \n87496      87497        4539   \n80420      80421        4143   \n140873    140874        7640   \n82718      82719        4272   \n119293    119294        6376   \n81445      81446        4199   \n9493        9494         395   \n147097    147098        8005   \n152017    152018        8295   \n79115      79116        4074   \n54839      54840        2731   \n73811      73812        3775   \n49423      49424        2419   \n11948      11949         512   \n8649        8650         359   \n140084    140085        7600   \n16330      16331         703   \n126023    126024        6770   \n59244      59245        2989   \n77412      77413        3982   \n\n                                                   Phrase  Sentiment  \n96584   that Skins comes as a welcome , if downbeat , ...          3  \n87200                         The Rock 's fighting skills          2  \n58251                        rolling over in their graves          0  \n6661                                   fact , even better          3  \n15383                                            sympathy          2  \n92337                                        for violence          1  \n78631   sustained fest of self-congratulation between ...          0  \n125370     characteristically complex Tom Clancy thriller          3  \n120529            truly edgy -- merely crassly flamboyant          2  \n43199   understands , in a way that speaks forcefully ...          2  \n110286                                          tolerable          2  \n33558   is made fresh by an intelligent screenplay and...          4  \n105339  the needs of moviegoers for real characters an...          1  \n119705  seems embarrassed by his own invention and tri...          1  \n17400   , Invincible shows he 's back in form , with a...          4  \n39025                                            best and          4  \n95315                                           as good ,          3  \n98463                                                 Coy          2  \n112535                         big-hearted and frequently          3  \n124336  keeps it fast -- zippy , comin ' at ya -- as i...          2  \n142156                                      take pictures          2  \n124574                           find in this dreary mess          1  \n83694   with few exceptions , it rarely stoops to chea...          3  \n119917                                 had a week to live          2  \n143267                                      broader ideas          2  \n23761                                            marginal          1  \n148501                                             Neeson          2  \n19786   The best part about `` Gangs '' was Daniel Day...          3  \n14995                                               vivid          3  \n148165                    this `` un-bear-able '' project          1  \n87496                          the brink of major changes          2  \n80420                 a far corner of the screen at times          2  \n140873             kids-and-family-oriented cable channel          3  \n82718                       's sincere to a fault , but ,          1  \n119293  for an absurd finale of twisted metal , fireba...          1  \n81445   the film -LRB- at 80 minutes -RRB- is actually...          3  \n9493                                              formula          2  \n147097              keep 80 minutes from seeming like 800          3  \n152017  The dirty jokes provide the funniest moments i...          3  \n79115                                      manipulation ,          1  \n54839   There 's an epic here , but you have to put it...          2  \n73811                            the characters are in it          2  \n49423                                countless filmmakers          2  \n11948                                              stance          2  \n8649                                          what is ...          2  \n140084                people who have never picked a lock          2  \n16330   great fun , full of the kind of energy it 's d...          4  \n126023                                           Powers ?          2  \n59244                                     collective fear          2  \n77412   ... could n't be more timely in its despairing...          2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>96584</th>\n      <td>96585</td>\n      <td>5045</td>\n      <td>that Skins comes as a welcome , if downbeat , ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>87200</th>\n      <td>87201</td>\n      <td>4523</td>\n      <td>The Rock 's fighting skills</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>58251</th>\n      <td>58252</td>\n      <td>2935</td>\n      <td>rolling over in their graves</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6661</th>\n      <td>6662</td>\n      <td>265</td>\n      <td>fact , even better</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>15383</th>\n      <td>15384</td>\n      <td>658</td>\n      <td>sympathy</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>92337</th>\n      <td>92338</td>\n      <td>4806</td>\n      <td>for violence</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>78631</th>\n      <td>78632</td>\n      <td>4047</td>\n      <td>sustained fest of self-congratulation between ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>125370</th>\n      <td>125371</td>\n      <td>6737</td>\n      <td>characteristically complex Tom Clancy thriller</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>120529</th>\n      <td>120530</td>\n      <td>6445</td>\n      <td>truly edgy -- merely crassly flamboyant</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>43199</th>\n      <td>43200</td>\n      <td>2087</td>\n      <td>understands , in a way that speaks forcefully ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>110286</th>\n      <td>110287</td>\n      <td>5840</td>\n      <td>tolerable</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>33558</th>\n      <td>33559</td>\n      <td>1574</td>\n      <td>is made fresh by an intelligent screenplay and...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>105339</th>\n      <td>105340</td>\n      <td>5562</td>\n      <td>the needs of moviegoers for real characters an...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>119705</th>\n      <td>119706</td>\n      <td>6403</td>\n      <td>seems embarrassed by his own invention and tri...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17400</th>\n      <td>17401</td>\n      <td>755</td>\n      <td>, Invincible shows he 's back in form , with a...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>39025</th>\n      <td>39026</td>\n      <td>1862</td>\n      <td>best and</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>95315</th>\n      <td>95316</td>\n      <td>4975</td>\n      <td>as good ,</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>98463</th>\n      <td>98464</td>\n      <td>5161</td>\n      <td>Coy</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>112535</th>\n      <td>112536</td>\n      <td>5977</td>\n      <td>big-hearted and frequently</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>124336</th>\n      <td>124337</td>\n      <td>6678</td>\n      <td>keeps it fast -- zippy , comin ' at ya -- as i...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>142156</th>\n      <td>142157</td>\n      <td>7711</td>\n      <td>take pictures</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>124574</th>\n      <td>124575</td>\n      <td>6693</td>\n      <td>find in this dreary mess</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>83694</th>\n      <td>83695</td>\n      <td>4326</td>\n      <td>with few exceptions , it rarely stoops to chea...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>119917</th>\n      <td>119918</td>\n      <td>6413</td>\n      <td>had a week to live</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>143267</th>\n      <td>143268</td>\n      <td>7777</td>\n      <td>broader ideas</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>23761</th>\n      <td>23762</td>\n      <td>1080</td>\n      <td>marginal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>148501</th>\n      <td>148502</td>\n      <td>8080</td>\n      <td>Neeson</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19786</th>\n      <td>19787</td>\n      <td>873</td>\n      <td>The best part about `` Gangs '' was Daniel Day...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>14995</th>\n      <td>14996</td>\n      <td>645</td>\n      <td>vivid</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>148165</th>\n      <td>148166</td>\n      <td>8062</td>\n      <td>this `` un-bear-able '' project</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87496</th>\n      <td>87497</td>\n      <td>4539</td>\n      <td>the brink of major changes</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>80420</th>\n      <td>80421</td>\n      <td>4143</td>\n      <td>a far corner of the screen at times</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>140873</th>\n      <td>140874</td>\n      <td>7640</td>\n      <td>kids-and-family-oriented cable channel</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>82718</th>\n      <td>82719</td>\n      <td>4272</td>\n      <td>'s sincere to a fault , but ,</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>119293</th>\n      <td>119294</td>\n      <td>6376</td>\n      <td>for an absurd finale of twisted metal , fireba...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>81445</th>\n      <td>81446</td>\n      <td>4199</td>\n      <td>the film -LRB- at 80 minutes -RRB- is actually...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9493</th>\n      <td>9494</td>\n      <td>395</td>\n      <td>formula</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>147097</th>\n      <td>147098</td>\n      <td>8005</td>\n      <td>keep 80 minutes from seeming like 800</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>152017</th>\n      <td>152018</td>\n      <td>8295</td>\n      <td>The dirty jokes provide the funniest moments i...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>79115</th>\n      <td>79116</td>\n      <td>4074</td>\n      <td>manipulation ,</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>54839</th>\n      <td>54840</td>\n      <td>2731</td>\n      <td>There 's an epic here , but you have to put it...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>73811</th>\n      <td>73812</td>\n      <td>3775</td>\n      <td>the characters are in it</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>49423</th>\n      <td>49424</td>\n      <td>2419</td>\n      <td>countless filmmakers</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11948</th>\n      <td>11949</td>\n      <td>512</td>\n      <td>stance</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8649</th>\n      <td>8650</td>\n      <td>359</td>\n      <td>what is ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>140084</th>\n      <td>140085</td>\n      <td>7600</td>\n      <td>people who have never picked a lock</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16330</th>\n      <td>16331</td>\n      <td>703</td>\n      <td>great fun , full of the kind of energy it 's d...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>126023</th>\n      <td>126024</td>\n      <td>6770</td>\n      <td>Powers ?</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>59244</th>\n      <td>59245</td>\n      <td>2989</td>\n      <td>collective fear</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>77412</th>\n      <td>77413</td>\n      <td>3982</td>\n      <td>... could n't be more timely in its despairing...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PhraseDataset(Dataset):\n    def __init__(self,df,pad_sequences):\n        super().__init__()\n        self.df = df\n        self.pad_sequences = pad_sequences\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        if 'Sentiment' in self.df.columns:\n            label = self.df['Sentiment'].values[idx]\n            item = self.pad_sequences[idx]\n            return item,label\n        else:\n            item = self.pad_sequences[idx]\n            return item","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentRNN(nn.Module):\n    \n    def __init__(self,corpus_size,output_size,embedd_dim,hidden_dim,n_layers):\n        super().__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(corpus_size,embedd_dim)\n        self.lstm = nn.LSTM(embedd_dim, hidden_dim,n_layers,dropout=0.5, batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim,output_size)\n        self.act = nn.Sigmoid()\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds,hidden)\n        lstm_out = lstm_out.contiguous().view(-1,self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        out = self.act(out)\n        out = out.view(batch_size,-1)\n        out = out[:,-5:]\n        return out, hidden\n    def init_hidden(self,batch_size):\n        \n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        return hidden","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(vocab_to_int)\noutput_size = 5\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\nnet = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim,n_layers)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net.train()\nclip=5\nepochs = 200\ncounter = 0\nprint_every = 100\nlr=0.01\n\ndef criterion(input, target, size_average=True):\n    \"\"\"Categorical cross-entropy with logits input and one-hot target\"\"\"\n    l = -(target * torch.log(F.softmax(input, dim=1) + 1e-10)).sum(1)\n    if size_average:\n        l = l.mean()\n    else:\n        l = l.sum()\n    return l\n\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=32\nlosses = []\naccs=[]\nfor e in range(epochs):\n    a = np.random.choice(len(train)-1, 1000)\n    train_set = PhraseDataset(train.loc[train.index.isin(np.sort(a))],pad_sequences[a])\n    train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n    h = net.init_hidden(32)\n    running_loss = 0.0\n    running_acc = 0.0\n\n    for idx,(inputs, labels) in enumerate(train_loader):\n        counter += 1\n        gc.collect()\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        optimizer.zero_grad()\n        if inputs.shape[0] != batch_size:\n            break\n        output, h = net(inputs, h)\n        labels=torch.nn.functional.one_hot(labels, num_classes=5)\n        loss = criterion(output, labels)\n        loss.backward()\n        running_loss += loss.cpu().detach().numpy()\n        running_acc += (output.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n        if idx%20 == 0:\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format((running_loss/(idx+1))))\n            losses.append(float(running_loss/(idx+1)))\n            print(f'acc:{running_acc/(idx+1)}')\n            accs.append(running_acc/(idx+1))","execution_count":43,"outputs":[{"output_type":"stream","text":"Epoch: 1/200... Step: 1... Loss: 1.615720...\nacc:0.0625\nEpoch: 1/200... Step: 21... Loss: 1.393763...\nacc:0.5059523582458496\nEpoch: 2/200... Step: 33... Loss: 1.185964...\nacc:0.71875\nEpoch: 2/200... Step: 53... Loss: 1.372548...\nacc:0.53125\nEpoch: 3/200... Step: 65... Loss: 1.395137...\nacc:0.5\nEpoch: 3/200... Step: 85... Loss: 1.377932...\nacc:0.5252976417541504\nEpoch: 4/200... Step: 97... Loss: 1.375176...\nacc:0.53125\nEpoch: 4/200... Step: 117... Loss: 1.376256...\nacc:0.5282738208770752\nEpoch: 5/200... Step: 129... Loss: 1.435468...\nacc:0.46875\nEpoch: 5/200... Step: 149... Loss: 1.414878...\nacc:0.4895833432674408\nEpoch: 6/200... Step: 161... Loss: 1.465887...\nacc:0.4375\nEpoch: 6/200... Step: 181... Loss: 1.385069...\nacc:0.5148809552192688\nEpoch: 7/200... Step: 192... Loss: 1.399219...\nacc:0.53125\nEpoch: 7/200... Step: 212... Loss: 1.391453...\nacc:0.511904776096344\nEpoch: 8/200... Step: 224... Loss: 1.284347...\nacc:0.5625\nEpoch: 8/200... Step: 244... Loss: 1.400521...\nacc:0.4955357015132904\nEpoch: 9/200... Step: 256... Loss: 1.373499...\nacc:0.53125\nEpoch: 9/200... Step: 276... Loss: 1.408386...\nacc:0.4955357015132904\nEpoch: 10/200... Step: 288... Loss: 1.319052...\nacc:0.59375\nEpoch: 10/200... Step: 308... Loss: 1.382164...\nacc:0.519345223903656\nEpoch: 11/200... Step: 320... Loss: 1.497963...\nacc:0.40625\nEpoch: 11/200... Step: 340... Loss: 1.405066...\nacc:0.4985119104385376\nEpoch: 12/200... Step: 352... Loss: 1.451957...\nacc:0.4375\nEpoch: 12/200... Step: 372... Loss: 1.384259...\nacc:0.511904776096344\nEpoch: 13/200... Step: 384... Loss: 1.373040...\nacc:0.53125\nEpoch: 13/200... Step: 404... Loss: 1.362633...\nacc:0.5401785969734192\nEpoch: 14/200... Step: 416... Loss: 1.435632...\nacc:0.46875\nEpoch: 14/200... Step: 436... Loss: 1.370646...\nacc:0.519345223903656\nEpoch: 15/200... Step: 448... Loss: 1.371434...\nacc:0.53125\nEpoch: 15/200... Step: 468... Loss: 1.400587...\nacc:0.5029761791229248\nEpoch: 16/200... Step: 480... Loss: 1.308017...\nacc:0.59375\nEpoch: 16/200... Step: 500... Loss: 1.384419...\nacc:0.519345223903656\nEpoch: 17/200... Step: 512... Loss: 1.310699...\nacc:0.59375\nEpoch: 17/200... Step: 532... Loss: 1.354854...\nacc:0.5491071343421936\nEpoch: 18/200... Step: 544... Loss: 1.335001...\nacc:0.5625\nEpoch: 18/200... Step: 564... Loss: 1.398978...\nacc:0.4851190447807312\nEpoch: 19/200... Step: 576... Loss: 1.411000...\nacc:0.59375\nEpoch: 19/200... Step: 596... Loss: 1.418404...\nacc:0.5\nEpoch: 20/200... Step: 608... Loss: 1.217995...\nacc:0.6875\nEpoch: 20/200... Step: 628... Loss: 1.392854...\nacc:0.511904776096344\nEpoch: 21/200... Step: 640... Loss: 1.529750...\nacc:0.375\nEpoch: 21/200... Step: 660... Loss: 1.406283...\nacc:0.4985119104385376\nEpoch: 22/200... Step: 672... Loss: 1.311080...\nacc:0.59375\nEpoch: 22/200... Step: 692... Loss: 1.413728...\nacc:0.4910714328289032\nEpoch: 23/200... Step: 704... Loss: 1.404776...\nacc:0.5\nEpoch: 23/200... Step: 724... Loss: 1.407793...\nacc:0.4970238208770752\nEpoch: 24/200... Step: 736... Loss: 1.436052...\nacc:0.46875\nEpoch: 24/200... Step: 756... Loss: 1.382492...\nacc:0.5223214030265808\nEpoch: 25/200... Step: 768... Loss: 1.404809...\nacc:0.5\nEpoch: 25/200... Step: 788... Loss: 1.361649...\nacc:0.543154776096344\nEpoch: 26/200... Step: 800... Loss: 1.342007...\nacc:0.5625\nEpoch: 26/200... Step: 820... Loss: 1.406259...\nacc:0.4985119104385376\nEpoch: 27/200... Step: 832... Loss: 1.373550...\nacc:0.53125\nEpoch: 27/200... Step: 852... Loss: 1.364641...\nacc:0.5401785969734192\nEpoch: 28/200... Step: 864... Loss: 1.373386...\nacc:0.53125\nEpoch: 28/200... Step: 884... Loss: 1.395861...\nacc:0.5089285969734192\nEpoch: 29/200... Step: 896... Loss: 1.560919...\nacc:0.34375\nEpoch: 29/200... Step: 916... Loss: 1.389912...\nacc:0.5148809552192688\nEpoch: 30/200... Step: 928... Loss: 1.498213...\nacc:0.40625\nEpoch: 30/200... Step: 948... Loss: 1.403304...\nacc:0.5014880895614624\nEpoch: 31/200... Step: 960... Loss: 1.342386...\nacc:0.5625\nEpoch: 31/200... Step: 980... Loss: 1.388376...\nacc:0.5163690447807312\nEpoch: 32/200... Step: 992... Loss: 1.342378...\nacc:0.5625\nEpoch: 32/200... Step: 1012... Loss: 1.433008...\nacc:0.4717261791229248\nEpoch: 33/200... Step: 1024... Loss: 1.404584...\nacc:0.5\nEpoch: 33/200... Step: 1044... Loss: 1.383818...\nacc:0.5208333134651184\nEpoch: 34/200... Step: 1056... Loss: 1.404766...\nacc:0.5\nEpoch: 34/200... Step: 1076... Loss: 1.403145...\nacc:0.5014880895614624\nEpoch: 35/200... Step: 1088... Loss: 1.371794...\nacc:0.53125\nEpoch: 35/200... Step: 1108... Loss: 1.392422...\nacc:0.511904776096344\nEpoch: 36/200... Step: 1119... Loss: 1.435370...\nacc:0.46875\nEpoch: 36/200... Step: 1139... Loss: 1.398636...\nacc:0.4955357015132904\nEpoch: 37/200... Step: 1151... Loss: 1.336405...\nacc:0.5625\nEpoch: 37/200... Step: 1171... Loss: 1.363943...\nacc:0.5401785969734192\nEpoch: 38/200... Step: 1183... Loss: 1.433871...\nacc:0.4375\nEpoch: 38/200... Step: 1203... Loss: 1.376233...\nacc:0.53125\nEpoch: 39/200... Step: 1215... Loss: 1.498209...\nacc:0.40625\nEpoch: 39/200... Step: 1235... Loss: 1.436028...\nacc:0.46875\nEpoch: 40/200... Step: 1247... Loss: 1.342365...\nacc:0.5625\nEpoch: 40/200... Step: 1267... Loss: 1.410650...\nacc:0.494047611951828\nEpoch: 41/200... Step: 1279... Loss: 1.436095...\nacc:0.46875\nEpoch: 41/200... Step: 1299... Loss: 1.392730...\nacc:0.511904776096344\nEpoch: 42/200... Step: 1311... Loss: 1.310714...\nacc:0.59375\nEpoch: 42/200... Step: 1331... Loss: 1.394316...\nacc:0.5104166865348816\nEpoch: 43/200... Step: 1343... Loss: 1.497266...\nacc:0.40625\nEpoch: 43/200... Step: 1363... Loss: 1.392427...\nacc:0.511904776096344\nEpoch: 44/200... Step: 1375... Loss: 1.376723...\nacc:0.53125\nEpoch: 44/200... Step: 1395... Loss: 1.372645...\nacc:0.53125\nEpoch: 45/200... Step: 1407... Loss: 1.461266...\nacc:0.46875\nEpoch: 45/200... Step: 1427... Loss: 1.406138...\nacc:0.5\nEpoch: 46/200... Step: 1439... Loss: 1.279917...\nacc:0.625\nEpoch: 46/200... Step: 1459... Loss: 1.376165...\nacc:0.5282738208770752\nEpoch: 47/200... Step: 1471... Loss: 1.528226...\nacc:0.375\nEpoch: 47/200... Step: 1491... Loss: 1.377092...\nacc:0.5267857313156128\nEpoch: 48/200... Step: 1503... Loss: 1.489379...\nacc:0.4375\nEpoch: 48/200... Step: 1523... Loss: 1.406534...\nacc:0.4791666567325592\nEpoch: 49/200... Step: 1535... Loss: 1.404287...\nacc:0.5\nEpoch: 49/200... Step: 1555... Loss: 1.373366...\nacc:0.53125\nEpoch: 50/200... Step: 1567... Loss: 1.373434...\nacc:0.53125\nEpoch: 50/200... Step: 1587... Loss: 1.413736...\nacc:0.4910714328289032\nEpoch: 51/200... Step: 1599... Loss: 1.498587...\nacc:0.40625\nEpoch: 51/200... Step: 1619... Loss: 1.383985...\nacc:0.5208333134651184\nEpoch: 52/200... Step: 1631... Loss: 1.342358...\nacc:0.5625\nEpoch: 52/200... Step: 1651... Loss: 1.388441...\nacc:0.5163690447807312\nEpoch: 53/200... Step: 1663... Loss: 1.467285...\nacc:0.4375\nEpoch: 53/200... Step: 1683... Loss: 1.395878...\nacc:0.5089285969734192\nEpoch: 54/200... Step: 1695... Loss: 1.498569...\nacc:0.40625\nEpoch: 54/200... Step: 1715... Loss: 1.361659...\nacc:0.543154776096344\nEpoch: 55/200... Step: 1727... Loss: 1.404817...\nacc:0.5\nEpoch: 55/200... Step: 1747... Loss: 1.372067...\nacc:0.5327380895614624\nEpoch: 56/200... Step: 1759... Loss: 1.498563...\nacc:0.40625\nEpoch: 56/200... Step: 1779... Loss: 1.388435...\nacc:0.5163690447807312\nEpoch: 57/200... Step: 1791... Loss: 1.436110...\nacc:0.46875\nEpoch: 57/200... Step: 1811... Loss: 1.398846...\nacc:0.5059523582458496\nEpoch: 58/200... Step: 1822... Loss: 1.279847...\nacc:0.625\nEpoch: 58/200... Step: 1842... Loss: 1.354196...\nacc:0.550595223903656\nEpoch: 59/200... Step: 1854... Loss: 1.342337...\nacc:0.5625\nEpoch: 59/200... Step: 1874... Loss: 1.360154...\nacc:0.5446428656578064\nEpoch: 60/200... Step: 1886... Loss: 1.311043...\nacc:0.59375\nEpoch: 60/200... Step: 1906... Loss: 1.385457...\nacc:0.519345223903656\nEpoch: 61/200... Step: 1918... Loss: 1.560953...\nacc:0.34375\nEpoch: 61/200... Step: 1938... Loss: 1.404795...\nacc:0.5\nEpoch: 62/200... Step: 1950... Loss: 1.436166...\nacc:0.46875\nEpoch: 62/200... Step: 1970... Loss: 1.358560...\nacc:0.5461309552192688\nEpoch: 63/200... Step: 1982... Loss: 1.467288...\nacc:0.4375\nEpoch: 63/200... Step: 2002... Loss: 1.406083...\nacc:0.4985119104385376\nEpoch: 64/200... Step: 2014... Loss: 1.436831...\nacc:0.46875\nEpoch: 64/200... Step: 2034... Loss: 1.377236...\nacc:0.5267857313156128\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 65/200... Step: 2046... Loss: 1.431434...\nacc:0.46875\nEpoch: 65/200... Step: 2066... Loss: 1.410873...\nacc:0.4955357015132904\nEpoch: 66/200... Step: 2078... Loss: 1.404484...\nacc:0.5\nEpoch: 66/200... Step: 2098... Loss: 1.392849...\nacc:0.511904776096344\nEpoch: 67/200... Step: 2110... Loss: 1.405183...\nacc:0.5\nEpoch: 67/200... Step: 2130... Loss: 1.425551...\nacc:0.4791666567325592\nEpoch: 68/200... Step: 2142... Loss: 1.217843...\nacc:0.6875\nEpoch: 68/200... Step: 2162... Loss: 1.351954...\nacc:0.5520833134651184\nEpoch: 69/200... Step: 2174... Loss: 1.442985...\nacc:0.4375\nEpoch: 69/200... Step: 2194... Loss: 1.435093...\nacc:0.4613095223903656\nEpoch: 70/200... Step: 2206... Loss: 1.371393...\nacc:0.53125\nEpoch: 70/200... Step: 2226... Loss: 1.398099...\nacc:0.5044642686843872\nEpoch: 71/200... Step: 2238... Loss: 1.488005...\nacc:0.40625\nEpoch: 71/200... Step: 2258... Loss: 1.360670...\nacc:0.5372023582458496\nEpoch: 72/200... Step: 2270... Loss: 1.424821...\nacc:0.46875\nEpoch: 72/200... Step: 2290... Loss: 1.348501...\nacc:0.5535714030265808\nEpoch: 73/200... Step: 2302... Loss: 1.438635...\nacc:0.46875\nEpoch: 73/200... Step: 2322... Loss: 1.362014...\nacc:0.5104166865348816\nEpoch: 74/200... Step: 2334... Loss: 1.418072...\nacc:0.34375\nEpoch: 74/200... Step: 2354... Loss: 1.406623...\nacc:0.494047611951828\nEpoch: 75/200... Step: 2366... Loss: 1.422360...\nacc:0.4375\nEpoch: 75/200... Step: 2386... Loss: 1.416326...\nacc:0.488095223903656\nEpoch: 76/200... Step: 2398... Loss: 1.216479...\nacc:0.6875\nEpoch: 76/200... Step: 2418... Loss: 1.400232...\nacc:0.5044642686843872\nEpoch: 77/200... Step: 2430... Loss: 1.435762...\nacc:0.46875\nEpoch: 77/200... Step: 2450... Loss: 1.357175...\nacc:0.5476190447807312\nEpoch: 78/200... Step: 2461... Loss: 1.216861...\nacc:0.6875\nEpoch: 78/200... Step: 2481... Loss: 1.403226...\nacc:0.5014880895614624\nEpoch: 79/200... Step: 2493... Loss: 1.248831...\nacc:0.65625\nEpoch: 79/200... Step: 2513... Loss: 1.361514...\nacc:0.543154776096344\nEpoch: 80/200... Step: 2525... Loss: 1.467223...\nacc:0.4375\nEpoch: 80/200... Step: 2545... Loss: 1.398904...\nacc:0.5059523582458496\nEpoch: 81/200... Step: 2557... Loss: 1.588424...\nacc:0.3125\nEpoch: 81/200... Step: 2577... Loss: 1.416126...\nacc:0.488095223903656\nEpoch: 82/200... Step: 2589... Loss: 1.345009...\nacc:0.5625\nEpoch: 82/200... Step: 2609... Loss: 1.394291...\nacc:0.511904776096344\nEpoch: 83/200... Step: 2621... Loss: 1.369945...\nacc:0.53125\nEpoch: 83/200... Step: 2641... Loss: 1.410201...\nacc:0.4866071343421936\nEpoch: 84/200... Step: 2653... Loss: 1.432572...\nacc:0.46875\nEpoch: 84/200... Step: 2673... Loss: 1.377614...\nacc:0.5267857313156128\nEpoch: 85/200... Step: 2685... Loss: 1.467402...\nacc:0.4375\nEpoch: 85/200... Step: 2705... Loss: 1.401828...\nacc:0.5029761791229248\nEpoch: 86/200... Step: 2717... Loss: 1.373410...\nacc:0.53125\nEpoch: 86/200... Step: 2737... Loss: 1.364552...\nacc:0.5401785969734192\nEpoch: 87/200... Step: 2749... Loss: 1.279351...\nacc:0.625\nEpoch: 87/200... Step: 2769... Loss: 1.351216...\nacc:0.5535714030265808\nEpoch: 88/200... Step: 2781... Loss: 1.404802...\nacc:0.5\nEpoch: 88/200... Step: 2801... Loss: 1.391423...\nacc:0.5133928656578064\nEpoch: 89/200... Step: 2813... Loss: 1.404808...\nacc:0.5\nEpoch: 89/200... Step: 2833... Loss: 1.398685...\nacc:0.5059523582458496\nEpoch: 90/200... Step: 2845... Loss: 1.373593...\nacc:0.53125\nEpoch: 90/200... Step: 2865... Loss: 1.420658...\nacc:0.4836309552192688\nEpoch: 91/200... Step: 2877... Loss: 1.431551...\nacc:0.46875\nEpoch: 91/200... Step: 2897... Loss: 1.393535...\nacc:0.507440447807312\nEpoch: 92/200... Step: 2909... Loss: 1.374038...\nacc:0.53125\nEpoch: 92/200... Step: 2929... Loss: 1.408675...\nacc:0.4955357015132904\nEpoch: 93/200... Step: 2941... Loss: 1.250089...\nacc:0.65625\nEpoch: 93/200... Step: 2961... Loss: 1.402816...\nacc:0.5\nEpoch: 94/200... Step: 2973... Loss: 1.440471...\nacc:0.46875\nEpoch: 94/200... Step: 2993... Loss: 1.397858...\nacc:0.4985119104385376\nEpoch: 95/200... Step: 3005... Loss: 1.410330...\nacc:0.5625\nEpoch: 95/200... Step: 3025... Loss: 1.402342...\nacc:0.519345223903656\nEpoch: 96/200... Step: 3037... Loss: 1.378173...\nacc:0.53125\nEpoch: 96/200... Step: 3057... Loss: 1.430552...\nacc:0.4583333432674408\nEpoch: 97/200... Step: 3069... Loss: 1.310920...\nacc:0.59375\nEpoch: 97/200... Step: 3089... Loss: 1.360012...\nacc:0.5446428656578064\nEpoch: 98/200... Step: 3101... Loss: 1.311133...\nacc:0.59375\nEpoch: 98/200... Step: 3121... Loss: 1.422654...\nacc:0.4821428656578064\nEpoch: 99/200... Step: 3133... Loss: 1.436008...\nacc:0.46875\nEpoch: 99/200... Step: 3153... Loss: 1.425606...\nacc:0.4791666567325592\nEpoch: 100/200... Step: 3165... Loss: 1.404806...\nacc:0.5\nEpoch: 100/200... Step: 3185... Loss: 1.389913...\nacc:0.5148809552192688\nEpoch: 101/200... Step: 3197... Loss: 1.436228...\nacc:0.46875\nEpoch: 101/200... Step: 3217... Loss: 1.407708...\nacc:0.4970238208770752\nEpoch: 102/200... Step: 3229... Loss: 1.311195...\nacc:0.59375\nEpoch: 102/200... Step: 3249... Loss: 1.385433...\nacc:0.519345223903656\nEpoch: 103/200... Step: 3261... Loss: 1.341568...\nacc:0.5625\nEpoch: 103/200... Step: 3281... Loss: 1.358314...\nacc:0.5461309552192688\nEpoch: 104/200... Step: 3293... Loss: 1.467149...\nacc:0.4375\nEpoch: 104/200... Step: 3313... Loss: 1.388708...\nacc:0.5133928656578064\nEpoch: 105/200... Step: 3325... Loss: 1.335339...\nacc:0.46875\nEpoch: 105/200... Step: 3345... Loss: 1.381596...\nacc:0.5252976417541504\nEpoch: 106/200... Step: 3357... Loss: 1.375088...\nacc:0.53125\nEpoch: 106/200... Step: 3377... Loss: 1.353225...\nacc:0.550595223903656\nEpoch: 107/200... Step: 3389... Loss: 1.303455...\nacc:0.5625\nEpoch: 107/200... Step: 3409... Loss: 1.373054...\nacc:0.53125\nEpoch: 108/200... Step: 3421... Loss: 1.344145...\nacc:0.5625\nEpoch: 108/200... Step: 3441... Loss: 1.395895...\nacc:0.5089285969734192\nEpoch: 109/200... Step: 3453... Loss: 1.342369...\nacc:0.5625\nEpoch: 109/200... Step: 3473... Loss: 1.411963...\nacc:0.4925595223903656\nEpoch: 110/200... Step: 3485... Loss: 1.373076...\nacc:0.53125\nEpoch: 110/200... Step: 3505... Loss: 1.395973...\nacc:0.5089285969734192\nEpoch: 111/200... Step: 3517... Loss: 1.217593...\nacc:0.6875\nEpoch: 111/200... Step: 3537... Loss: 1.389783...\nacc:0.5104166865348816\nEpoch: 112/200... Step: 3549... Loss: 1.581604...\nacc:0.34375\nEpoch: 112/200... Step: 3569... Loss: 1.405491...\nacc:0.5014880895614624\nEpoch: 113/200... Step: 3581... Loss: 1.404417...\nacc:0.5\nEpoch: 113/200... Step: 3601... Loss: 1.384507...\nacc:0.5208333134651184\nEpoch: 114/200... Step: 3613... Loss: 1.373869...\nacc:0.53125\nEpoch: 114/200... Step: 3633... Loss: 1.415278...\nacc:0.4776785671710968\nEpoch: 115/200... Step: 3645... Loss: 1.481057...\nacc:0.5\nEpoch: 115/200... Step: 3665... Loss: 1.377988...\nacc:0.5148809552192688\nEpoch: 116/200... Step: 3677... Loss: 1.383211...\nacc:0.53125\nEpoch: 116/200... Step: 3697... Loss: 1.398865...\nacc:0.5044642686843872\nEpoch: 117/200... Step: 3709... Loss: 1.283055...\nacc:0.65625\nEpoch: 117/200... Step: 3729... Loss: 1.403782...\nacc:0.4925595223903656\nEpoch: 118/200... Step: 3741... Loss: 1.466701...\nacc:0.4375\nEpoch: 118/200... Step: 3761... Loss: 1.406675...\nacc:0.5044642686843872\nEpoch: 119/200... Step: 3773... Loss: 1.279931...\nacc:0.625\nEpoch: 119/200... Step: 3793... Loss: 1.395841...\nacc:0.5089285969734192\nEpoch: 120/200... Step: 3805... Loss: 1.436090...\nacc:0.46875\nEpoch: 120/200... Step: 3825... Loss: 1.401859...\nacc:0.5029761791229248\nEpoch: 121/200... Step: 3837... Loss: 1.467381...\nacc:0.4375\nEpoch: 121/200... Step: 3857... Loss: 1.400329...\nacc:0.5044642686843872\nEpoch: 122/200... Step: 3869... Loss: 1.342358...\nacc:0.5625\nEpoch: 122/200... Step: 3889... Loss: 1.372085...\nacc:0.5327380895614624\nEpoch: 123/200... Step: 3901... Loss: 1.404804...\nacc:0.5\nEpoch: 123/200... Step: 3921... Loss: 1.364601...\nacc:0.5401785969734192\nEpoch: 124/200... Step: 3933... Loss: 1.279573...\nacc:0.625\nEpoch: 124/200... Step: 3953... Loss: 1.371992...\nacc:0.5327380895614624\nEpoch: 125/200... Step: 3965... Loss: 1.435785...\nacc:0.46875\nEpoch: 125/200... Step: 3985... Loss: 1.416706...\nacc:0.488095223903656\nEpoch: 126/200... Step: 3997... Loss: 1.373693...\nacc:0.53125\nEpoch: 126/200... Step: 4017... Loss: 1.415149...\nacc:0.4895833432674408\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 127/200... Step: 4029... Loss: 1.467637...\nacc:0.4375\nEpoch: 127/200... Step: 4049... Loss: 1.391148...\nacc:0.5133928656578064\nEpoch: 128/200... Step: 4061... Loss: 1.279876...\nacc:0.625\nEpoch: 128/200... Step: 4081... Loss: 1.413608...\nacc:0.4910714328289032\nEpoch: 129/200... Step: 4093... Loss: 1.311272...\nacc:0.59375\nEpoch: 129/200... Step: 4113... Loss: 1.366159...\nacc:0.538690447807312\nEpoch: 130/200... Step: 4125... Loss: 1.310872...\nacc:0.59375\nEpoch: 130/200... Step: 4145... Loss: 1.398157...\nacc:0.5059523582458496\nEpoch: 131/200... Step: 4157... Loss: 1.373784...\nacc:0.53125\nEpoch: 131/200... Step: 4177... Loss: 1.399990...\nacc:0.5029761791229248\nEpoch: 132/200... Step: 4189... Loss: 1.469207...\nacc:0.4375\nEpoch: 132/200... Step: 4209... Loss: 1.370046...\nacc:0.5297619104385376\nEpoch: 133/200... Step: 4221... Loss: 1.368869...\nacc:0.5\nEpoch: 133/200... Step: 4241... Loss: 1.365590...\nacc:0.5342261791229248\nEpoch: 134/200... Step: 4253... Loss: 1.342473...\nacc:0.5625\nEpoch: 134/200... Step: 4273... Loss: 1.376369...\nacc:0.5282738208770752\nEpoch: 135/200... Step: 4285... Loss: 1.467437...\nacc:0.4375\nEpoch: 135/200... Step: 4305... Loss: 1.413196...\nacc:0.4910714328289032\nEpoch: 136/200... Step: 4317... Loss: 1.497294...\nacc:0.40625\nEpoch: 136/200... Step: 4337... Loss: 1.398557...\nacc:0.5044642686843872\nEpoch: 137/200... Step: 4349... Loss: 1.479526...\nacc:0.40625\nEpoch: 137/200... Step: 4369... Loss: 1.394083...\nacc:0.5059523582458496\nEpoch: 138/200... Step: 4381... Loss: 1.308926...\nacc:0.625\nEpoch: 138/200... Step: 4401... Loss: 1.389252...\nacc:0.5252976417541504\nEpoch: 139/200... Step: 4413... Loss: 1.343022...\nacc:0.5625\nEpoch: 139/200... Step: 4433... Loss: 1.420960...\nacc:0.480654776096344\nEpoch: 140/200... Step: 4445... Loss: 1.338922...\nacc:0.5625\nEpoch: 140/200... Step: 4465... Loss: 1.399807...\nacc:0.5223214030265808\nEpoch: 141/200... Step: 4477... Loss: 1.335585...\nacc:0.5625\nEpoch: 141/200... Step: 4497... Loss: 1.381013...\nacc:0.5133928656578064\nEpoch: 142/200... Step: 4509... Loss: 1.374799...\nacc:0.53125\nEpoch: 142/200... Step: 4529... Loss: 1.399603...\nacc:0.5\nEpoch: 143/200... Step: 4541... Loss: 1.362448...\nacc:0.5625\nEpoch: 143/200... Step: 4561... Loss: 1.393924...\nacc:0.5044642686843872\nEpoch: 144/200... Step: 4573... Loss: 1.493215...\nacc:0.46875\nEpoch: 144/200... Step: 4593... Loss: 1.411765...\nacc:0.494047611951828\nEpoch: 145/200... Step: 4605... Loss: 1.409313...\nacc:0.5\nEpoch: 145/200... Step: 4625... Loss: 1.394022...\nacc:0.511904776096344\nEpoch: 146/200... Step: 4637... Loss: 1.592009...\nacc:0.3125\nEpoch: 146/200... Step: 4657... Loss: 1.413747...\nacc:0.4910714328289032\nEpoch: 147/200... Step: 4669... Loss: 1.342302...\nacc:0.5625\nEpoch: 147/200... Step: 4689... Loss: 1.406278...\nacc:0.4985119104385376\nEpoch: 148/200... Step: 4701... Loss: 1.342331...\nacc:0.5625\nEpoch: 148/200... Step: 4721... Loss: 1.382546...\nacc:0.5223214030265808\nEpoch: 149/200... Step: 4733... Loss: 1.279863...\nacc:0.625\nEpoch: 149/200... Step: 4753... Loss: 1.413750...\nacc:0.4910714328289032\nEpoch: 150/200... Step: 4765... Loss: 1.436100...\nacc:0.46875\nEpoch: 150/200... Step: 4785... Loss: 1.398884...\nacc:0.5059523582458496\nEpoch: 151/200... Step: 4797... Loss: 1.560859...\nacc:0.34375\nEpoch: 151/200... Step: 4817... Loss: 1.406293...\nacc:0.4985119104385376\nEpoch: 152/200... Step: 4829... Loss: 1.279944...\nacc:0.625\nEpoch: 152/200... Step: 4849... Loss: 1.406321...\nacc:0.4985119104385376\nEpoch: 153/200... Step: 4861... Loss: 1.248593...\nacc:0.65625\nEpoch: 153/200... Step: 4881... Loss: 1.387002...\nacc:0.5178571343421936\nEpoch: 154/200... Step: 4893... Loss: 1.561026...\nacc:0.34375\nEpoch: 154/200... Step: 4913... Loss: 1.363188...\nacc:0.5416666865348816\nEpoch: 155/200... Step: 4925... Loss: 1.498490...\nacc:0.40625\nEpoch: 155/200... Step: 4945... Loss: 1.413679...\nacc:0.4910714328289032\nEpoch: 156/200... Step: 4957... Loss: 1.498565...\nacc:0.40625\nEpoch: 156/200... Step: 4977... Loss: 1.345243...\nacc:0.5595238208770752\nEpoch: 157/200... Step: 4989... Loss: 1.561036...\nacc:0.34375\nEpoch: 157/200... Step: 5009... Loss: 1.385449...\nacc:0.519345223903656\nEpoch: 158/200... Step: 5021... Loss: 1.280438...\nacc:0.625\nEpoch: 158/200... Step: 5041... Loss: 1.378014...\nacc:0.5267857313156128\nEpoch: 159/200... Step: 5053... Loss: 1.342456...\nacc:0.5625\nEpoch: 159/200... Step: 5073... Loss: 1.389940...\nacc:0.5148809552192688\nEpoch: 160/200... Step: 5085... Loss: 1.373565...\nacc:0.53125\nEpoch: 160/200... Step: 5105... Loss: 1.389853...\nacc:0.5148809552192688\nEpoch: 161/200... Step: 5117... Loss: 1.310015...\nacc:0.59375\nEpoch: 161/200... Step: 5137... Loss: 1.403185...\nacc:0.5014880895614624\nEpoch: 162/200... Step: 5149... Loss: 1.591900...\nacc:0.3125\nEpoch: 162/200... Step: 5169... Loss: 1.416918...\nacc:0.4866071343421936\nEpoch: 163/200... Step: 5181... Loss: 1.430643...\nacc:0.4375\nEpoch: 163/200... Step: 5201... Loss: 1.413262...\nacc:0.5029761791229248\nEpoch: 164/200... Step: 5213... Loss: 1.444816...\nacc:0.5625\nEpoch: 164/200... Step: 5233... Loss: 1.401745...\nacc:0.5342261791229248\nEpoch: 165/200... Step: 5245... Loss: 1.288633...\nacc:0.6875\nEpoch: 165/200... Step: 5265... Loss: 1.419646...\nacc:0.4985119104385376\nEpoch: 166/200... Step: 5277... Loss: 1.383476...\nacc:0.46875\nEpoch: 166/200... Step: 5297... Loss: 1.390723...\nacc:0.5297619104385376\nEpoch: 167/200... Step: 5309... Loss: 1.400871...\nacc:0.6875\nEpoch: 167/200... Step: 5329... Loss: 1.377018...\nacc:0.53125\nEpoch: 168/200... Step: 5341... Loss: 1.388918...\nacc:0.40625\nEpoch: 168/200... Step: 5361... Loss: 1.389683...\nacc:0.5059523582458496\nEpoch: 169/200... Step: 5373... Loss: 1.394150...\nacc:0.5\nEpoch: 169/200... Step: 5393... Loss: 1.399957...\nacc:0.5044642686843872\nEpoch: 170/200... Step: 5405... Loss: 1.561272...\nacc:0.34375\nEpoch: 170/200... Step: 5425... Loss: 1.434334...\nacc:0.4702380895614624\nEpoch: 171/200... Step: 5437... Loss: 1.342041...\nacc:0.5625\nEpoch: 171/200... Step: 5457... Loss: 1.388418...\nacc:0.5163690447807312\nEpoch: 172/200... Step: 5469... Loss: 1.467884...\nacc:0.4375\nEpoch: 172/200... Step: 5489... Loss: 1.406754...\nacc:0.494047611951828\nEpoch: 173/200... Step: 5501... Loss: 1.285037...\nacc:0.65625\nEpoch: 173/200... Step: 5521... Loss: 1.382107...\nacc:0.5208333134651184\nEpoch: 174/200... Step: 5533... Loss: 1.463282...\nacc:0.4375\nEpoch: 174/200... Step: 5553... Loss: 1.425891...\nacc:0.4761904776096344\nEpoch: 175/200... Step: 5565... Loss: 1.449162...\nacc:0.4375\nEpoch: 175/200... Step: 5585... Loss: 1.413183...\nacc:0.4821428656578064\nEpoch: 176/200... Step: 5597... Loss: 1.285208...\nacc:0.65625\nEpoch: 176/200... Step: 5617... Loss: 1.343652...\nacc:0.5565476417541504\nEpoch: 177/200... Step: 5629... Loss: 1.512385...\nacc:0.375\nEpoch: 177/200... Step: 5649... Loss: 1.371427...\nacc:0.5327380895614624\nEpoch: 178/200... Step: 5661... Loss: 1.356226...\nacc:0.53125\nEpoch: 178/200... Step: 5681... Loss: 1.399791...\nacc:0.5044642686843872\nEpoch: 179/200... Step: 5693... Loss: 1.433208...\nacc:0.46875\nEpoch: 179/200... Step: 5713... Loss: 1.385361...\nacc:0.511904776096344\nEpoch: 180/200... Step: 5725... Loss: 1.505358...\nacc:0.40625\nEpoch: 180/200... Step: 5745... Loss: 1.427222...\nacc:0.4821428656578064\nEpoch: 181/200... Step: 5757... Loss: 1.424105...\nacc:0.5\nEpoch: 181/200... Step: 5777... Loss: 1.380433...\nacc:0.5163690447807312\nEpoch: 182/200... Step: 5789... Loss: 1.273096...\nacc:0.625\nEpoch: 182/200... Step: 5809... Loss: 1.385760...\nacc:0.5178571343421936\nEpoch: 183/200... Step: 5821... Loss: 1.374643...\nacc:0.53125\nEpoch: 183/200... Step: 5841... Loss: 1.386465...\nacc:0.5163690447807312\nEpoch: 184/200... Step: 5853... Loss: 1.383079...\nacc:0.46875\nEpoch: 184/200... Step: 5873... Loss: 1.446373...\nacc:0.4672619104385376\nEpoch: 185/200... Step: 5885... Loss: 1.566815...\nacc:0.34375\nEpoch: 185/200... Step: 5905... Loss: 1.389096...\nacc:0.5163690447807312\nEpoch: 186/200... Step: 5917... Loss: 1.342580...\nacc:0.5625\nEpoch: 186/200... Step: 5937... Loss: 1.401220...\nacc:0.5029761791229248\nEpoch: 187/200... Step: 5949... Loss: 1.374871...\nacc:0.53125\nEpoch: 187/200... Step: 5969... Loss: 1.397171...\nacc:0.507440447807312\nEpoch: 188/200... Step: 5981... Loss: 1.190257...\nacc:0.71875\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 188/200... Step: 6001... Loss: 1.401396...\nacc:0.5014880895614624\nEpoch: 189/200... Step: 6013... Loss: 1.471490...\nacc:0.40625\nEpoch: 189/200... Step: 6033... Loss: 1.400318...\nacc:0.507440447807312\nEpoch: 190/200... Step: 6045... Loss: 1.092496...\nacc:0.8125\nEpoch: 190/200... Step: 6065... Loss: 1.379598...\nacc:0.5327380895614624\nEpoch: 191/200... Step: 6077... Loss: 1.403026...\nacc:0.5\nEpoch: 191/200... Step: 6097... Loss: 1.383033...\nacc:0.519345223903656\nEpoch: 192/200... Step: 6109... Loss: 1.329665...\nacc:0.5625\nEpoch: 192/200... Step: 6129... Loss: 1.392921...\nacc:0.5044642686843872\nEpoch: 193/200... Step: 6141... Loss: 1.379810...\nacc:0.5\nEpoch: 193/200... Step: 6161... Loss: 1.393473...\nacc:0.507440447807312\nEpoch: 194/200... Step: 6173... Loss: 1.383736...\nacc:0.53125\nEpoch: 194/200... Step: 6193... Loss: 1.394698...\nacc:0.5104166865348816\nEpoch: 195/200... Step: 6205... Loss: 1.436604...\nacc:0.46875\nEpoch: 195/200... Step: 6225... Loss: 1.413411...\nacc:0.4910714328289032\nEpoch: 196/200... Step: 6237... Loss: 1.375147...\nacc:0.53125\nEpoch: 196/200... Step: 6257... Loss: 1.401701...\nacc:0.5014880895614624\nEpoch: 197/200... Step: 6269... Loss: 1.287178...\nacc:0.625\nEpoch: 197/200... Step: 6289... Loss: 1.387250...\nacc:0.5267857313156128\nEpoch: 198/200... Step: 6301... Loss: 1.349387...\nacc:0.5\nEpoch: 198/200... Step: 6321... Loss: 1.377425...\nacc:0.5342261791229248\nEpoch: 199/200... Step: 6333... Loss: 1.561831...\nacc:0.34375\nEpoch: 199/200... Step: 6353... Loss: 1.429127...\nacc:0.474702388048172\nEpoch: 200/200... Step: 6365... Loss: 1.404988...\nacc:0.5\nEpoch: 200/200... Step: 6385... Loss: 1.361490...\nacc:0.543154776096344\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}