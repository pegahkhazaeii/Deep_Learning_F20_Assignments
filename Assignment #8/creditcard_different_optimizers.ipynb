{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0F6F1MHHKLnS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2v-TEOpXKPm5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tY7ZInGSKYwd",
    "outputId": "6b075626-3f9f-4dbe-843a-108001a386f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QUCkRh4iKbrp"
   },
   "outputs": [],
   "source": [
    "data  = pd.read_csv(\"/content/drive/My Drive/Deep Learning/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8oYNfKPKy82",
    "outputId": "56f1d64d-cbcc-4d26-f165-bfa84c7fe1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled train dataset shape Counter({0: 227454, 1: 391})\n",
      "Sampled validation dataset shape Counter({0: 56861, 1: 101})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "\n",
    "# Original dataset\n",
    "x = data.drop('Class', axis=1).values\n",
    "y = data['Class'].values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Sampled train dataset shape %s' % Counter(ytrain))\n",
    "print('Sampled validation dataset shape %s' % Counter(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a0C_OcwqK1WE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "bs =100\n",
    "\n",
    "#creating torch dataset and loader using original dataset. \n",
    "#to use resampled dataset, replace ex. xtrain with xtrain_over etc.\n",
    "train_ds = torch.utils.data.TensorDataset(torch.tensor(xtrain).float(), torch.tensor(ytrain).float())\n",
    "valid_ds = torch.utils.data.TensorDataset(torch.tensor(xtest).float(), torch.tensor(ytest).float())\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=bs)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xeutQHbwK3ws"
   },
   "outputs": [],
   "source": [
    "#network class 2-hidden layer model\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, n_input=10, n_hidden = 20, n_output = 1,drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.extractor1 = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.extractor2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.drop_out = torch.nn.Dropout(drop_prob)\n",
    "        self.classifier = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        x = self.relu(self.extractor1(xb))\n",
    "        x = self.relu(self.extractor2(x))\n",
    "        x = self.drop_out(x)\n",
    "        return self.classifier(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D9rdAJCeK5zl"
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "J2uKJhYqK9Le"
   },
   "outputs": [],
   "source": [
    "#training the network\n",
    "def train(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZmHAGR7b3cp"
   },
   "source": [
    "## **SGD Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "n7KKt6_Xbktv"
   },
   "outputs": [],
   "source": [
    "#network setting\n",
    "n_input = xtrain.shape[1]\n",
    "n_output = 1\n",
    "n_hidden = 15\n",
    "\n",
    "model = Classifier(n_input=n_input,n_hidden=n_hidden,n_output=n_output,drop_prob=0.2)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#for orignal dataset, I use pos_weight.\n",
    "pos_weight = torch.tensor([5])\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "n_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QCk9uiTkbozu",
    "outputId": "ef3f67b7-fe29-400b-8563-af5268568a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.04767648543390339\n",
      "1 0.04698426333979026\n",
      "2 0.04617386487997381\n",
      "3 0.04530260065999299\n",
      "4 0.04416965013509873\n",
      "5 0.04280117842718375\n",
      "6 0.040858868309784385\n",
      "7 0.03829832972613636\n",
      "8 0.03551580954672629\n",
      "9 0.0329761027580929\n",
      "10 0.030442962878180664\n",
      "11 0.028063188695068332\n",
      "12 0.025926954272382962\n",
      "13 0.024147200849551288\n",
      "14 0.022663339682841337\n",
      "15 0.021387844870019938\n",
      "16 0.020280538461459535\n",
      "17 0.01928858037371332\n",
      "18 0.018332927306053355\n",
      "19 0.01753875197289306\n",
      "20 0.016763873888428798\n",
      "21 0.016038949297289176\n",
      "22 0.01537617374636925\n",
      "23 0.014840396827639497\n",
      "24 0.014352141504105428\n",
      "25 0.01376061488499419\n",
      "26 0.013355276332199581\n",
      "27 0.013090116663226648\n",
      "28 0.012833250318067166\n",
      "29 0.012545347633299708\n",
      "30 0.012452679449926956\n",
      "31 0.01232564876780285\n",
      "32 0.012119109895251707\n",
      "33 0.012082000394206397\n",
      "34 0.011950621133601002\n",
      "35 0.011907963315043365\n",
      "36 0.011803857742935209\n",
      "37 0.01164326720366817\n",
      "38 0.011596404041378641\n",
      "39 0.011455302346470683\n",
      "40 0.011373100145541486\n",
      "41 0.011474861171426945\n",
      "42 0.011356284858371708\n",
      "43 0.011273782646366009\n",
      "44 0.011169232825110583\n",
      "45 0.011294252769732942\n",
      "46 0.011134970404279907\n",
      "47 0.011096883884233124\n",
      "48 0.011096357919569992\n",
      "49 0.011067268379058248\n",
      "50 0.010935957205002874\n",
      "51 0.010892816237856583\n",
      "52 0.01098931691898667\n",
      "53 0.010871170030529016\n",
      "54 0.01097067501394592\n",
      "55 0.011074294565848991\n",
      "56 0.010870268841296712\n",
      "57 0.010837490585927607\n",
      "58 0.01084356395921152\n",
      "59 0.010925481509569505\n",
      "60 0.010721711202418067\n",
      "61 0.01075022039364594\n",
      "62 0.01066622271622844\n",
      "63 0.010908805199939597\n",
      "64 0.01076207964789125\n",
      "65 0.010772948163543832\n",
      "66 0.010698910856009691\n",
      "67 0.010614669459868449\n",
      "68 0.010871978695287608\n",
      "69 0.010615346296816461\n",
      "70 0.010605316900057848\n",
      "71 0.01065413268177336\n",
      "72 0.0106179116093465\n",
      "73 0.010622813225739327\n",
      "74 0.010752615160728684\n",
      "75 0.010559907268346455\n",
      "76 0.01052937151589967\n",
      "77 0.010625812987944062\n",
      "78 0.010602627772832229\n",
      "79 0.010528954820985013\n",
      "80 0.01051992103175783\n",
      "81 0.010601659137107318\n",
      "82 0.010558413942925072\n",
      "83 0.010454308803905129\n",
      "84 0.010510503181996244\n",
      "85 0.01045836097700075\n",
      "86 0.010419087990166819\n",
      "87 0.010644073206268815\n",
      "88 0.01046309656057383\n",
      "89 0.010394231237292547\n",
      "90 0.010518966252802837\n",
      "91 0.010496268960408405\n",
      "92 0.010550188060983337\n",
      "93 0.010311674078376958\n",
      "94 0.010649034888526665\n",
      "95 0.01031449389427323\n",
      "96 0.010417366731277775\n",
      "97 0.01061935309875203\n",
      "98 0.010353346130123443\n",
      "99 0.010348640688946425\n",
      "100 0.010392049216974593\n",
      "101 0.010406801894924737\n",
      "102 0.01038617709637275\n",
      "103 0.010301748139779146\n",
      "104 0.010392790056607728\n",
      "105 0.010360486480628966\n",
      "106 0.0103732844376432\n",
      "107 0.010349935792202685\n",
      "108 0.010490031952059392\n",
      "109 0.010502633875344674\n",
      "110 0.010598468691613606\n",
      "111 0.010381635436322598\n",
      "112 0.010479200252069729\n",
      "113 0.010274384059552546\n",
      "114 0.010394494842619393\n",
      "115 0.010348798264083294\n",
      "116 0.010249289350559968\n",
      "117 0.010225054223166209\n",
      "118 0.01030629977978731\n",
      "119 0.010336054275307089\n",
      "120 0.010455826068561111\n",
      "121 0.01042531939041177\n",
      "122 0.010319122048469322\n",
      "123 0.010225615803958896\n",
      "124 0.010453740497911174\n",
      "125 0.010239196662129114\n",
      "126 0.01019581967967646\n",
      "127 0.010286932209014327\n",
      "128 0.010498321286321277\n",
      "129 0.010440585070327538\n",
      "130 0.01024104279698424\n",
      "131 0.010185170097541992\n",
      "132 0.01022753940305639\n",
      "133 0.010179004884345168\n",
      "134 0.010228034078152669\n",
      "135 0.0102203733710799\n",
      "136 0.010181679617840769\n",
      "137 0.010252926913989867\n",
      "138 0.010135903504569142\n",
      "139 0.010339300898476437\n",
      "140 0.010311055186372632\n",
      "141 0.010257605797170008\n",
      "142 0.01026790701531044\n",
      "143 0.010071355998625709\n",
      "144 0.010283265832810671\n",
      "145 0.010266154241673606\n",
      "146 0.0102217297140785\n",
      "147 0.01017645305462622\n",
      "148 0.010232199761860484\n",
      "149 0.010203935726349422\n",
      "150 0.010379744231782066\n",
      "151 0.010074913186086434\n",
      "152 0.010171850899393523\n",
      "153 0.010288706966713504\n",
      "154 0.010255346673693993\n",
      "155 0.010101790407328833\n",
      "156 0.010229322579409209\n",
      "157 0.010205073729340198\n",
      "158 0.01020220135903526\n",
      "159 0.01012852456101277\n",
      "160 0.010218581626277768\n",
      "161 0.010048573984223612\n",
      "162 0.010070984020644463\n",
      "163 0.010061919683153531\n",
      "164 0.010021966734310366\n",
      "165 0.010159380901697308\n",
      "166 0.010157999429763162\n",
      "167 0.010043358760313485\n",
      "168 0.010040807654873312\n",
      "169 0.010047014606667286\n",
      "170 0.010094072522378823\n",
      "171 0.010192092999783266\n",
      "172 0.010024987181791036\n",
      "173 0.010067538903517555\n",
      "174 0.010055692590259138\n",
      "175 0.010052810390063676\n",
      "176 0.01008827878394932\n",
      "177 0.010186681849364738\n",
      "178 0.01009335611286946\n",
      "179 0.010023115574444163\n",
      "180 0.010029164306217567\n",
      "181 0.01009161143927898\n",
      "182 0.010007861916349878\n",
      "183 0.01000471417044788\n",
      "184 0.01004439354521404\n",
      "185 0.01004194756627974\n",
      "186 0.010133028422408704\n",
      "187 0.010032170036185168\n",
      "188 0.010133817193119883\n",
      "189 0.009974721306650464\n",
      "190 0.010077732284726209\n",
      "191 0.010040409420494506\n",
      "192 0.010189847781151984\n",
      "193 0.01000857004648931\n",
      "194 0.010103041140508518\n",
      "195 0.010061362090086332\n",
      "196 0.010080203183883543\n",
      "197 0.010007761881829988\n",
      "198 0.01000728347739051\n",
      "199 0.00990555610520504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (extractor1): Linear(in_features=30, out_features=15, bias=True)\n",
       "  (extractor2): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (drop_out): Dropout(p=0.2, inplace=False)\n",
       "  (classifier): Linear(in_features=15, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(n_epoch,model,loss_func,opt,train_dl,valid_dl)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WW8kTXsmgYC5",
    "outputId": "296dabdc-88dc-4ba1-aff9-d47bce47d36e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9993679997191109\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "ypred = model(torch.tensor(xtest).float()).detach().numpy()\n",
    "\n",
    "ypred [ypred>=0.5] =1.0\n",
    "ypred [ypred<0.5] =0.0\n",
    "print('Accuracy score: {}'.format(metrics.accuracy_score(ytest, ypred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nG3XzS_LB96"
   },
   "source": [
    "## **Adam Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ARFmGAOrK_vg"
   },
   "outputs": [],
   "source": [
    "#network setting\n",
    "n_input = xtrain.shape[1]\n",
    "n_output = 1\n",
    "n_hidden = 15\n",
    "\n",
    "model = Classifier(n_input=n_input,n_hidden=n_hidden,n_output=n_output,drop_prob=0.2)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#for orignal dataset, I use pos_weight.\n",
    "pos_weight = torch.tensor([5])\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "n_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJyVWv54LJIa",
    "outputId": "b097f967-a01d-4bf5-d6ce-a58091947004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03662987152521263\n",
      "1 0.02021260144687296\n",
      "2 0.015076272979464112\n",
      "3 0.013930225812321532\n",
      "4 0.013237360732705171\n",
      "5 0.012510836359147397\n",
      "6 0.012706423001131378\n",
      "7 0.01202770463282336\n",
      "8 0.01173016331146173\n",
      "9 0.011450087165358442\n",
      "10 0.011302948844006662\n",
      "11 0.011379850824401183\n",
      "12 0.011520987088905584\n",
      "13 0.011213159758370856\n",
      "14 0.010953839029804922\n",
      "15 0.011256639363325983\n",
      "16 0.01118905194579839\n",
      "17 0.011021117039799095\n",
      "18 0.011410597640127688\n",
      "19 0.010908506795602792\n",
      "20 0.010863967755535393\n",
      "21 0.01087764661972398\n",
      "22 0.010851942810280167\n",
      "23 0.010884245584629833\n",
      "24 0.011616970355056776\n",
      "25 0.010667660189423906\n",
      "26 0.010935514817206048\n",
      "27 0.010758664649199636\n",
      "28 0.0106092322493461\n",
      "29 0.010738325311108741\n",
      "30 0.011677314331279179\n",
      "31 0.01068805483918364\n",
      "32 0.010679457417932362\n",
      "33 0.010837597549059821\n",
      "34 0.010728327871443235\n",
      "35 0.010825119003069622\n",
      "36 0.010586002631651041\n",
      "37 0.0110795744159198\n",
      "38 0.010857426464417414\n",
      "39 0.010733470387744333\n",
      "40 0.01055711125981166\n",
      "41 0.010597754461082003\n",
      "42 0.010843133699606535\n",
      "43 0.010607805490376186\n",
      "44 0.010499183828924452\n",
      "45 0.010946364192848841\n",
      "46 0.010694549505434877\n",
      "47 0.010950969872552238\n",
      "48 0.011099835526425057\n",
      "49 0.010671277631709448\n",
      "50 0.010764742330822254\n",
      "51 0.010761291814195474\n",
      "52 0.010592163165735496\n",
      "53 0.01091660661613869\n",
      "54 0.010803150509508702\n",
      "55 0.010292643496670239\n",
      "56 0.011067852515114655\n",
      "57 0.010950586601553118\n",
      "58 0.010572341279729067\n",
      "59 0.010418138818507588\n",
      "60 0.01049331398049169\n",
      "61 0.010667796199731133\n",
      "62 0.010856393346872764\n",
      "63 0.010787521472981005\n",
      "64 0.010416507002385842\n",
      "65 0.0108955531143709\n",
      "66 0.010672596887434584\n",
      "67 0.010689778674906681\n",
      "68 0.010413547066276152\n",
      "69 0.010514926361839426\n",
      "70 0.010622508370170272\n",
      "71 0.010538266947167177\n",
      "72 0.01043990249049206\n",
      "73 0.010415603538416501\n",
      "74 0.010500655018692398\n",
      "75 0.010344558269036153\n",
      "76 0.01059434207476599\n",
      "77 0.010407524030763213\n",
      "78 0.010249591406587008\n",
      "79 0.010383669517029749\n",
      "80 0.010226816886253157\n",
      "81 0.010634068817552087\n",
      "82 0.010539601056434445\n",
      "83 0.010435490961032481\n",
      "84 0.010568559803734324\n",
      "85 0.010203777375299794\n",
      "86 0.0103707099236655\n",
      "87 0.010275997593593432\n",
      "88 0.010402680018206266\n",
      "89 0.010196148537539607\n",
      "90 0.010604343776474968\n",
      "91 0.010406984147083059\n",
      "92 0.010186235704510794\n",
      "93 0.010426818052847822\n",
      "94 0.010389138896751339\n",
      "95 0.01052890753978556\n",
      "96 0.01058234711297361\n",
      "97 0.01005818143634702\n",
      "98 0.010346223760061658\n",
      "99 0.010644979839647437\n",
      "100 0.0104903989860922\n",
      "101 0.010275839625742364\n",
      "102 0.010261909593983551\n",
      "103 0.010158539330944854\n",
      "104 0.010398421230615205\n",
      "105 0.01023936080899592\n",
      "106 0.010430917218126869\n",
      "107 0.01024929593286011\n",
      "108 0.010505401393281365\n",
      "109 0.010315435251499312\n",
      "110 0.010353568606339835\n",
      "111 0.010210327496931338\n",
      "112 0.0102530670266527\n",
      "113 0.009935234858414292\n",
      "114 0.009951574298910528\n",
      "115 0.01019141236896022\n",
      "116 0.009979890597432255\n",
      "117 0.00991332024601085\n",
      "118 0.010222434569955769\n",
      "119 0.009986053260679414\n",
      "120 0.010220557233950434\n",
      "121 0.01002469192135053\n",
      "122 0.010212350262622344\n",
      "123 0.010215932821473726\n",
      "124 0.01037686366111703\n",
      "125 0.010203928550113378\n",
      "126 0.01026621546709855\n",
      "127 0.009849747020228916\n",
      "128 0.010040818457464924\n",
      "129 0.010283610810653073\n",
      "130 0.010121984661525085\n",
      "131 0.010125563133735664\n",
      "132 0.010267908772379736\n",
      "133 0.009919777696740068\n",
      "134 0.010167151055643066\n",
      "135 0.010112718196981806\n",
      "136 0.010088987831754057\n",
      "137 0.0106605278289412\n",
      "138 0.010034694766779447\n",
      "139 0.01039298312155335\n",
      "140 0.009836612747309256\n",
      "141 0.010196993060802137\n",
      "142 0.010858711925563199\n",
      "143 0.009904453207473036\n",
      "144 0.010192826275797245\n",
      "145 0.010660698617975833\n",
      "146 0.009970793176880169\n",
      "147 0.00995564636525344\n",
      "148 0.009933480731476423\n",
      "149 0.009961536330556084\n",
      "150 0.009862696177448628\n",
      "151 0.009693270115414454\n",
      "152 0.009961715977825013\n",
      "153 0.009962795643237609\n",
      "154 0.010231151737350426\n",
      "155 0.01008724009622827\n",
      "156 0.010127884833530647\n",
      "157 0.010073511505046023\n",
      "158 0.010089617960692038\n",
      "159 0.010059332254888314\n",
      "160 0.009983711384472328\n",
      "161 0.009881727423518921\n",
      "162 0.010162818783675935\n",
      "163 0.010282523142641525\n",
      "164 0.009994354980284248\n",
      "165 0.010198443325462174\n",
      "166 0.010353990989161357\n",
      "167 0.00983810587700812\n",
      "168 0.009861561655291711\n",
      "169 0.010065720284140177\n",
      "170 0.010024461354412843\n",
      "171 0.00973605440688224\n",
      "172 0.009913432992775995\n",
      "173 0.009866594018179652\n",
      "174 0.010438168746507328\n",
      "175 0.009883471078941306\n",
      "176 0.009755045688530244\n",
      "177 0.010192212061919696\n",
      "178 0.009853108082904051\n",
      "179 0.009894725128459002\n",
      "180 0.010195253434129165\n",
      "181 0.009945563178319902\n",
      "182 0.009833852812279996\n",
      "183 0.00988480759902883\n",
      "184 0.010076407174777993\n",
      "185 0.009719789947405685\n",
      "186 0.010089554770686498\n",
      "187 0.009747358585415483\n",
      "188 0.009710963026751271\n",
      "189 0.009880106306919573\n",
      "190 0.010010069507000913\n",
      "191 0.009872720525892331\n",
      "192 0.009918857113747131\n",
      "193 0.009877296300555652\n",
      "194 0.01039667364791646\n",
      "195 0.01017916771719564\n",
      "196 0.010370452797262513\n",
      "197 0.010317978829244618\n",
      "198 0.009636589153374986\n",
      "199 0.009656505842563144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (extractor1): Linear(in_features=30, out_features=15, bias=True)\n",
       "  (extractor2): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (drop_out): Dropout(p=0.2, inplace=False)\n",
       "  (classifier): Linear(in_features=15, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(n_epoch,model,loss_func,opt,train_dl,valid_dl)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEy_nVE3LLkG",
    "outputId": "31999683-0d2a-4d2d-9510-8786894f50ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.999420666409185\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "ypred = model(torch.tensor(xtest).float()).detach().numpy()\n",
    "\n",
    "ypred [ypred>=0.5] =1.0\n",
    "ypred [ypred<0.5] =0.0\n",
    "print('Accuracy score: {}'.format(metrics.accuracy_score(ytest, ypred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaSpaviSiKc7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzFD75wPiKyp"
   },
   "source": [
    "## **Adadelta Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AFc1QL9NLOT7"
   },
   "outputs": [],
   "source": [
    "#network setting\n",
    "n_input = xtrain.shape[1]\n",
    "n_output = 1\n",
    "n_hidden = 15\n",
    "\n",
    "model = Classifier(n_input=n_input,n_hidden=n_hidden,n_output=n_output,drop_prob=0.2)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#for orignal dataset, I use pos_weight.\n",
    "pos_weight = torch.tensor([5])\n",
    "opt = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "n_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3hOcDJ-LQKf",
    "outputId": "3bc384df-409c-4440-e813-7f0760bb22e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7054230957142419\n",
      "1 0.2845288438599605\n",
      "2 0.09200966939142273\n",
      "3 0.06185265763205514\n",
      "4 0.052121967718759626\n",
      "5 0.04885232249429651\n",
      "6 0.047719709172641886\n",
      "7 0.04724647675913826\n",
      "8 0.047064826455931116\n",
      "9 0.047019943317049784\n",
      "10 0.0470503481146793\n",
      "11 0.047118375471369225\n",
      "12 0.04721022915026879\n",
      "13 0.047320854219890435\n",
      "14 0.0474264008025873\n",
      "15 0.04753349471436971\n",
      "16 0.04763944248549593\n",
      "17 0.04774206306609485\n",
      "18 0.04783716202919852\n",
      "19 0.04792985050262432\n",
      "20 0.04802050806220576\n",
      "21 0.048097096198278295\n",
      "22 0.048178109290584276\n",
      "23 0.048252546557101254\n",
      "24 0.048313107722742034\n",
      "25 0.048371700265258\n",
      "26 0.04842555623997824\n",
      "27 0.04847926571747928\n",
      "28 0.04851866364234961\n",
      "29 0.04856836294185644\n",
      "30 0.048600940864809435\n",
      "31 0.04863269112527183\n",
      "32 0.04866613262607458\n",
      "33 0.04869306297796841\n",
      "34 0.04871537384232211\n",
      "35 0.04873455274821976\n",
      "36 0.04875660230654834\n",
      "37 0.04878337112861681\n",
      "38 0.04880970797280061\n",
      "39 0.04880934083025101\n",
      "40 0.04881823959980993\n",
      "41 0.04882403169602967\n",
      "42 0.04883696848149013\n",
      "43 0.04885063167132804\n",
      "44 0.048852331059392046\n",
      "45 0.04884513485262116\n",
      "46 0.04884527111526547\n",
      "47 0.048839454607307424\n",
      "48 0.04882994870951531\n",
      "49 0.048833488443093875\n",
      "50 0.048825126182617744\n",
      "51 0.0488287769453894\n",
      "52 0.048826022822353495\n",
      "53 0.04882448089923727\n",
      "54 0.048810506989723716\n",
      "55 0.048792380720730714\n",
      "56 0.04878661469293485\n",
      "57 0.04877689854540097\n",
      "58 0.04878158428306507\n",
      "59 0.04876291071419105\n",
      "60 0.04874462321744513\n",
      "61 0.04872976355354548\n",
      "62 0.04870680016818806\n",
      "63 0.048682002513393934\n",
      "64 0.04866544678384287\n",
      "65 0.04864791688725725\n",
      "66 0.048627340901605566\n",
      "67 0.0486070206450225\n",
      "68 0.048588569701892306\n",
      "69 0.04856568831854583\n",
      "70 0.048546009987050796\n",
      "71 0.04852059839936173\n",
      "72 0.04850049456216053\n",
      "73 0.048478766600943905\n",
      "74 0.04845632045081927\n",
      "75 0.0484275983830759\n",
      "76 0.048406770445123516\n",
      "77 0.04838526681595819\n",
      "78 0.04836400791278443\n",
      "79 0.04834955753562842\n",
      "80 0.04834417044305274\n",
      "81 0.04832412687381145\n",
      "82 0.048303238623860725\n",
      "83 0.048291660564028845\n",
      "84 0.04826287091011369\n",
      "85 0.0482409504688479\n",
      "86 0.048217377994676285\n",
      "87 0.04820253535820047\n",
      "88 0.04819465654843039\n",
      "89 0.048170332551844\n",
      "90 0.04815053075015481\n",
      "91 0.048132934755605504\n",
      "92 0.0481042407940385\n",
      "93 0.04808260817911638\n",
      "94 0.048055824254178085\n",
      "95 0.04803690399347501\n",
      "96 0.04801964425633223\n",
      "97 0.047994136316535985\n",
      "98 0.04797588121922747\n",
      "99 0.047944577260667906\n",
      "100 0.047937453759800554\n",
      "101 0.04791719228027158\n",
      "102 0.04789065253376696\n",
      "103 0.04786670963652157\n",
      "104 0.04783887707316746\n",
      "105 0.04781492239404241\n",
      "106 0.04779803477489319\n",
      "107 0.04777003760247307\n",
      "108 0.047752009031428656\n",
      "109 0.047731028277252535\n",
      "110 0.04770870612558707\n",
      "111 0.04767520080044486\n",
      "112 0.04765711191219465\n",
      "113 0.04763007978869478\n",
      "114 0.047611842723282095\n",
      "115 0.047593935667229935\n",
      "116 0.04756097679818348\n",
      "117 0.04754108093287339\n",
      "118 0.04751859093431549\n",
      "119 0.047488566342104785\n",
      "120 0.047474836179755606\n",
      "121 0.04745355571481781\n",
      "122 0.04742863835457212\n",
      "123 0.04740815882799679\n",
      "124 0.047388656941680006\n",
      "125 0.04736314395798281\n",
      "126 0.04735235024029344\n",
      "127 0.04733088325788336\n",
      "128 0.047307425563275884\n",
      "129 0.04729124196731616\n",
      "130 0.04726177443739281\n",
      "131 0.04723471482956021\n",
      "132 0.047219703362566076\n",
      "133 0.04719067125123964\n",
      "134 0.04717367471091347\n",
      "135 0.0471380095938423\n",
      "136 0.04711522845622363\n",
      "137 0.047100059776061874\n",
      "138 0.04707370331779497\n",
      "139 0.04705823081545401\n",
      "140 0.0470383300686955\n",
      "141 0.04701451113253343\n",
      "142 0.04698920757549989\n",
      "143 0.04696763956977747\n",
      "144 0.04694416460023331\n",
      "145 0.04692237573792353\n",
      "146 0.04689394643014476\n",
      "147 0.046878098491525994\n",
      "148 0.04686021397388235\n",
      "149 0.04684363570374006\n",
      "150 0.046812601863642334\n",
      "151 0.04679331394504922\n",
      "152 0.046767116346637536\n",
      "153 0.046750859077302094\n",
      "154 0.04673842544341602\n",
      "155 0.046713711689942615\n",
      "156 0.04669395498563862\n",
      "157 0.046664660890665774\n",
      "158 0.046643805979985145\n",
      "159 0.046621898323779366\n",
      "160 0.046601427878375755\n",
      "161 0.0465875536590697\n",
      "162 0.046557966625011014\n",
      "163 0.04653738013779069\n",
      "164 0.04651944144781944\n",
      "165 0.04649398356833649\n",
      "166 0.04646885554405093\n",
      "167 0.04645069758653837\n",
      "168 0.046428635047645946\n",
      "169 0.04640522207290464\n",
      "170 0.04638092696213106\n",
      "171 0.04635572988794142\n",
      "172 0.04632191722181248\n",
      "173 0.04630464824379224\n",
      "174 0.04629526161434549\n",
      "175 0.04625677711612782\n",
      "176 0.04622450707471352\n",
      "177 0.046191293834162706\n",
      "178 0.0461654907836879\n",
      "179 0.04613517559327086\n",
      "180 0.04610944375670649\n",
      "181 0.04608525932871267\n",
      "182 0.046053426681117615\n",
      "183 0.046030465113492194\n",
      "184 0.04601761857525257\n",
      "185 0.045999087526134894\n",
      "186 0.045978203450180916\n",
      "187 0.04594954537000617\n",
      "188 0.04591943665018405\n",
      "189 0.04589538651682252\n",
      "190 0.04587075987719481\n",
      "191 0.04585598715378294\n",
      "192 0.04583169473577943\n",
      "193 0.045814637761419315\n",
      "194 0.04579227141248746\n",
      "195 0.045761823120244544\n",
      "196 0.04573286509279057\n",
      "197 0.04571070314267703\n",
      "198 0.04570576873813321\n",
      "199 0.045685122168556955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (extractor1): Linear(in_features=30, out_features=15, bias=True)\n",
       "  (extractor2): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (drop_out): Dropout(p=0.2, inplace=False)\n",
       "  (classifier): Linear(in_features=15, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(n_epoch,model,loss_func,opt,train_dl,valid_dl)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3FfXO6AlLUI",
    "outputId": "4f4626c6-b97d-45e5-82d0-58455bcf5e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9982268881008391\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "ypred = model(torch.tensor(xtest).float()).detach().numpy()\n",
    "\n",
    "ypred [ypred>=0.5] =1.0\n",
    "ypred [ypred<0.5] =0.0\n",
    "print('Accuracy score: {}'.format(metrics.accuracy_score(ytest, ypred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlIF3s4ulMCM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0qaV4Wep8m4"
   },
   "source": [
    "## **RMSprop Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_nZBbRBHlMUY"
   },
   "outputs": [],
   "source": [
    "#network setting\n",
    "n_input = xtrain.shape[1]\n",
    "n_output = 1\n",
    "n_hidden = 15\n",
    "\n",
    "model = Classifier(n_input=n_input,n_hidden=n_hidden,n_output=n_output,drop_prob=0.2)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#for orignal dataset, I use pos_weight.\n",
    "pos_weight = torch.tensor([5])\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "n_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJluweBrp3_c",
    "outputId": "9a1ef3f9-be2e-457b-9b51-7436d5b8c8b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03496223439070151\n",
      "1 0.013979871606656076\n",
      "2 0.011933266703498275\n",
      "3 0.011239341047338687\n",
      "4 0.011133184239051274\n",
      "5 0.010929863881706886\n",
      "6 0.010783810782130485\n",
      "7 0.010751321248073637\n",
      "8 0.010691884435692413\n",
      "9 0.010708625443484861\n",
      "10 0.010541012739448508\n",
      "11 0.010764422175900592\n",
      "12 0.010744914971521734\n",
      "13 0.010706968073505102\n",
      "14 0.010520363501651555\n",
      "15 0.010980050398253408\n",
      "16 0.010756790727825915\n",
      "17 0.010843587952522367\n",
      "18 0.010396848590688281\n",
      "19 0.01042071140703407\n",
      "20 0.010633501436743365\n",
      "21 0.010383688508532442\n",
      "22 0.010328687332602443\n",
      "23 0.010548790703902418\n",
      "24 0.010616510880416017\n",
      "25 0.010510644784142455\n",
      "26 0.010556447321001557\n",
      "27 0.011011127048109956\n",
      "28 0.01055728723234518\n",
      "29 0.010650470361661245\n",
      "30 0.010466485411878746\n",
      "31 0.010365644323355027\n",
      "32 0.010443524542407016\n",
      "33 0.010275810057730286\n",
      "34 0.010264244164493078\n",
      "35 0.010113414968118774\n",
      "36 0.010117366277271613\n",
      "37 0.010137995488401044\n",
      "38 0.01010131217719658\n",
      "39 0.010630002302154523\n",
      "40 0.010605573032376131\n",
      "41 0.010501500919412422\n",
      "42 0.010269618948970872\n",
      "43 0.010450660208246315\n",
      "44 0.010548492552406481\n",
      "45 0.01037387900305025\n",
      "46 0.01041557761265055\n",
      "47 0.010224784139218998\n",
      "48 0.01105768399358641\n",
      "49 0.010455952587981597\n",
      "50 0.010182232766899899\n",
      "51 0.010103789834302995\n",
      "52 0.010362716411866568\n",
      "53 0.01015806142151574\n",
      "54 0.010765134159074078\n",
      "55 0.010420725005911001\n",
      "56 0.011049603120544916\n",
      "57 0.010470623696740261\n",
      "58 0.010713032264711889\n",
      "59 0.010767690523771896\n",
      "60 0.010596227021256425\n",
      "61 0.011171073299928256\n",
      "62 0.010246267521284486\n",
      "63 0.011004660315435913\n",
      "64 0.01064588905598105\n",
      "65 0.010578340821021845\n",
      "66 0.010595723895453562\n",
      "67 0.010828337606006834\n",
      "68 0.010913048516223676\n",
      "69 0.010555336157149165\n",
      "70 0.010508582333738522\n",
      "71 0.010626380314593254\n",
      "72 0.010441196158714168\n",
      "73 0.01050961615889528\n",
      "74 0.010927414235176973\n",
      "75 0.010443665800049752\n",
      "76 0.010643791158141664\n",
      "77 0.01011074598106674\n",
      "78 0.011427766737530607\n",
      "79 0.011125490684426314\n",
      "80 0.011641982057847177\n",
      "81 0.010021019771824081\n",
      "82 0.0108738231521267\n",
      "83 0.0100341359011853\n",
      "84 0.010587628650261353\n",
      "85 0.01071581349879818\n",
      "86 0.009856974059793577\n",
      "87 0.010390526610547145\n",
      "88 0.010751301854296146\n",
      "89 0.012019051837175347\n",
      "90 0.010877378482264166\n",
      "91 0.010493679988392473\n",
      "92 0.010746718956226349\n",
      "93 0.010793851502933962\n",
      "94 0.010451802257736356\n",
      "95 0.010507218940438415\n",
      "96 0.01071998599405411\n",
      "97 0.010577515729018423\n",
      "98 0.010411566557044187\n",
      "99 0.010802809438083262\n",
      "100 0.010857981087918023\n",
      "101 0.010835216026857377\n",
      "102 0.010553891814315439\n",
      "103 0.010347208491296165\n",
      "104 0.010321228421530935\n",
      "105 0.010444280768403818\n",
      "106 0.01061267132766373\n",
      "107 0.010916822086198794\n",
      "108 0.010950743536579944\n",
      "109 0.010618344015076178\n",
      "110 0.01038240293933073\n",
      "111 0.01070736982728921\n",
      "112 0.010832912930964318\n",
      "113 0.010819272617807553\n",
      "114 0.011040026001925845\n",
      "115 0.010482188410324554\n",
      "116 0.010546813942442878\n",
      "117 0.010260308913816308\n",
      "118 0.01060647770576439\n",
      "119 0.01064820606379068\n",
      "120 0.01125568124171467\n",
      "121 0.010200054862115383\n",
      "122 0.010722778455888482\n",
      "123 0.010369253013905896\n",
      "124 0.010112939000901561\n",
      "125 0.01203622727498788\n",
      "126 0.010761292588340401\n",
      "127 0.010520180513829821\n",
      "128 0.010168706023333989\n",
      "129 0.010398042098646544\n",
      "130 0.010453668883401795\n",
      "131 0.010965216088518267\n",
      "132 0.010557129301474504\n",
      "133 0.010432613565890566\n",
      "134 0.01097135848678289\n",
      "135 0.010608999132892021\n",
      "136 0.010423634635772309\n",
      "137 0.01028082428733365\n",
      "138 0.010002650050380665\n",
      "139 0.010754395505813544\n",
      "140 0.010894265227685359\n",
      "141 0.011648784361130081\n",
      "142 0.01038261566921017\n",
      "143 0.010715560703605352\n",
      "144 0.010917302162719295\n",
      "145 0.010942607540080888\n",
      "146 0.010065995308355056\n",
      "147 0.010538730511207794\n",
      "148 0.010485618634109604\n",
      "149 0.010961852240457058\n",
      "150 0.010192292426518541\n",
      "151 0.010967795131882662\n",
      "152 0.010574281821856338\n",
      "153 0.010340543691254206\n",
      "154 0.010134322045199573\n",
      "155 0.010295343420394067\n",
      "156 0.010405113089092572\n",
      "157 0.011170990136643721\n",
      "158 0.010986657959711486\n",
      "159 0.01066274151985514\n",
      "160 0.010440574910356189\n",
      "161 0.010315416359895482\n",
      "162 0.010204746065916401\n",
      "163 0.010480099595964267\n",
      "164 0.010141399527237561\n",
      "165 0.010238387943458795\n",
      "166 0.01105565238704462\n",
      "167 0.011001801255678574\n",
      "168 0.01032027818917141\n",
      "169 0.010863639291072119\n",
      "170 0.010527649869249424\n",
      "171 0.01042870808303689\n",
      "172 0.01053707856820288\n",
      "173 0.01043141633271047\n",
      "174 0.00988977417324706\n",
      "175 0.009932689428257847\n",
      "176 0.010820753012871007\n",
      "177 0.010553788756412266\n",
      "178 0.010219808678364153\n",
      "179 0.010487090095366492\n",
      "180 0.010558068600626193\n",
      "181 0.010168839398775317\n",
      "182 0.010085154541252769\n",
      "183 0.010067636795509727\n",
      "184 0.010527484127517542\n",
      "185 0.010811938902247489\n",
      "186 0.010388033562145524\n",
      "187 0.010250964901788756\n",
      "188 0.010197860296191812\n",
      "189 0.010161482565643478\n",
      "190 0.010961923050540487\n",
      "191 0.011361560444333592\n",
      "192 0.010159163230353083\n",
      "193 0.010083654515482977\n",
      "194 0.009710551346098275\n",
      "195 0.010467825260305702\n",
      "196 0.010318212842294101\n",
      "197 0.01023881884275128\n",
      "198 0.010365471843022629\n",
      "199 0.010363020284337027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (extractor1): Linear(in_features=30, out_features=15, bias=True)\n",
       "  (extractor2): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (drop_out): Dropout(p=0.2, inplace=False)\n",
       "  (classifier): Linear(in_features=15, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(n_epoch,model,loss_func,opt,train_dl,valid_dl)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0qy9jBGp7ed",
    "outputId": "90876062-6eca-44c7-863b-dbfbe6cc8aa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9994382219725431\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "ypred = model(torch.tensor(xtest).float()).detach().numpy()\n",
    "\n",
    "ypred [ypred>=0.5] =1.0\n",
    "ypred [ypred<0.5] =0.0\n",
    "print('Accuracy score: {}'.format(metrics.accuracy_score(ytest, ypred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuUQd1Cxt8_P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "creditcard-different-optimizers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
