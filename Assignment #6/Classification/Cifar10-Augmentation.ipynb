{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0rK9kseCmqg",
        "outputId": "a18b80f6-9fca-4ccd-b4a9-0022f4c4b9ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        " \n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate\n",
        " \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2rM1WcAsnNK"
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        " \n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        " \n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUkJV0HMsnEn"
      },
      "source": [
        "weight_decay = 1e-4\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtfuiUcLw2DT",
        "outputId": "9f5212c5-f32f-4e38-f766-f2580974fb26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajUEAWU0xHtH",
        "outputId": "86b8f971-4df0-46bc-8c6c-df3f6003b939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        " \n",
        "#training\n",
        "batch_size = 64\n",
        " \n",
        "opt_rms = keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "#save to disk\n",
        "model_json = model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights('model.h5') \n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 1.8863 - accuracy: 0.4278 - val_loss: 1.3983 - val_accuracy: 0.5596\n",
            "Epoch 2/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 1.2575 - accuracy: 0.5865 - val_loss: 1.0583 - val_accuracy: 0.6752\n",
            "Epoch 3/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 1.0670 - accuracy: 0.6561 - val_loss: 0.9196 - val_accuracy: 0.7197\n",
            "Epoch 4/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.9600 - accuracy: 0.6961 - val_loss: 0.8294 - val_accuracy: 0.7492\n",
            "Epoch 5/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8948 - accuracy: 0.7203 - val_loss: 0.7941 - val_accuracy: 0.7672\n",
            "Epoch 6/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8516 - accuracy: 0.7376 - val_loss: 0.7822 - val_accuracy: 0.7734\n",
            "Epoch 7/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8173 - accuracy: 0.7522 - val_loss: 0.7507 - val_accuracy: 0.7864\n",
            "Epoch 8/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7899 - accuracy: 0.7620 - val_loss: 0.6799 - val_accuracy: 0.8022\n",
            "Epoch 9/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7718 - accuracy: 0.7724 - val_loss: 0.7196 - val_accuracy: 0.8023\n",
            "Epoch 10/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7520 - accuracy: 0.7810 - val_loss: 0.7038 - val_accuracy: 0.8052\n",
            "Epoch 11/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.7298 - accuracy: 0.7887 - val_loss: 0.7614 - val_accuracy: 0.7845\n",
            "Epoch 12/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7220 - accuracy: 0.7916 - val_loss: 0.7149 - val_accuracy: 0.8018\n",
            "Epoch 13/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7112 - accuracy: 0.7989 - val_loss: 0.7338 - val_accuracy: 0.7991\n",
            "Epoch 14/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7038 - accuracy: 0.8019 - val_loss: 0.6762 - val_accuracy: 0.8183\n",
            "Epoch 15/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.6978 - accuracy: 0.8032 - val_loss: 0.6801 - val_accuracy: 0.8199\n",
            "Epoch 16/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6908 - accuracy: 0.8071 - val_loss: 0.6314 - val_accuracy: 0.8346\n",
            "Epoch 17/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6829 - accuracy: 0.8113 - val_loss: 0.6394 - val_accuracy: 0.8293\n",
            "Epoch 18/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6840 - accuracy: 0.8094 - val_loss: 0.7011 - val_accuracy: 0.8161\n",
            "Epoch 19/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6652 - accuracy: 0.8168 - val_loss: 0.6808 - val_accuracy: 0.8190\n",
            "Epoch 20/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6673 - accuracy: 0.8173 - val_loss: 0.6785 - val_accuracy: 0.8253\n",
            "Epoch 21/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6614 - accuracy: 0.8195 - val_loss: 0.6420 - val_accuracy: 0.8293\n",
            "Epoch 22/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6592 - accuracy: 0.8208 - val_loss: 0.7223 - val_accuracy: 0.8094\n",
            "Epoch 23/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6514 - accuracy: 0.8259 - val_loss: 0.6009 - val_accuracy: 0.8448\n",
            "Epoch 24/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6488 - accuracy: 0.8247 - val_loss: 0.6307 - val_accuracy: 0.8362\n",
            "Epoch 25/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6458 - accuracy: 0.8274 - val_loss: 0.6450 - val_accuracy: 0.8318\n",
            "Epoch 26/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6402 - accuracy: 0.8279 - val_loss: 0.6125 - val_accuracy: 0.8394\n",
            "Epoch 27/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6380 - accuracy: 0.8295 - val_loss: 0.6156 - val_accuracy: 0.8431\n",
            "Epoch 28/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6398 - accuracy: 0.8312 - val_loss: 0.6042 - val_accuracy: 0.8474\n",
            "Epoch 29/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6338 - accuracy: 0.8326 - val_loss: 0.5794 - val_accuracy: 0.8544\n",
            "Epoch 30/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6296 - accuracy: 0.8327 - val_loss: 0.6510 - val_accuracy: 0.8378\n",
            "Epoch 31/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6277 - accuracy: 0.8331 - val_loss: 0.6178 - val_accuracy: 0.8455\n",
            "Epoch 32/125\n",
            "781/781 [==============================] - 25s 31ms/step - loss: 0.6207 - accuracy: 0.8362 - val_loss: 0.5899 - val_accuracy: 0.8525\n",
            "Epoch 33/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6255 - accuracy: 0.8358 - val_loss: 0.6066 - val_accuracy: 0.8480\n",
            "Epoch 34/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6210 - accuracy: 0.8375 - val_loss: 0.5908 - val_accuracy: 0.8525\n",
            "Epoch 35/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6140 - accuracy: 0.8402 - val_loss: 0.6348 - val_accuracy: 0.8415\n",
            "Epoch 36/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6167 - accuracy: 0.8389 - val_loss: 0.6237 - val_accuracy: 0.8478\n",
            "Epoch 37/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6141 - accuracy: 0.8395 - val_loss: 0.5847 - val_accuracy: 0.8528\n",
            "Epoch 38/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6130 - accuracy: 0.8413 - val_loss: 0.6061 - val_accuracy: 0.8512\n",
            "Epoch 39/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6105 - accuracy: 0.8425 - val_loss: 0.6069 - val_accuracy: 0.8470\n",
            "Epoch 40/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6093 - accuracy: 0.8432 - val_loss: 0.5551 - val_accuracy: 0.8613\n",
            "Epoch 41/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6098 - accuracy: 0.8423 - val_loss: 0.6278 - val_accuracy: 0.8456\n",
            "Epoch 42/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6066 - accuracy: 0.8432 - val_loss: 0.5787 - val_accuracy: 0.8605\n",
            "Epoch 43/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5993 - accuracy: 0.8458 - val_loss: 0.5913 - val_accuracy: 0.8533\n",
            "Epoch 44/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6015 - accuracy: 0.8450 - val_loss: 0.5807 - val_accuracy: 0.8593\n",
            "Epoch 45/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6038 - accuracy: 0.8454 - val_loss: 0.5960 - val_accuracy: 0.8535\n",
            "Epoch 46/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5972 - accuracy: 0.8479 - val_loss: 0.5721 - val_accuracy: 0.8603\n",
            "Epoch 47/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6028 - accuracy: 0.8445 - val_loss: 0.5803 - val_accuracy: 0.8600\n",
            "Epoch 48/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5997 - accuracy: 0.8460 - val_loss: 0.6903 - val_accuracy: 0.8289\n",
            "Epoch 49/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6001 - accuracy: 0.8467 - val_loss: 0.6907 - val_accuracy: 0.8284\n",
            "Epoch 50/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6000 - accuracy: 0.8467 - val_loss: 0.5850 - val_accuracy: 0.8580\n",
            "Epoch 51/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6000 - accuracy: 0.8459 - val_loss: 0.6120 - val_accuracy: 0.8518\n",
            "Epoch 52/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5989 - accuracy: 0.8469 - val_loss: 0.5942 - val_accuracy: 0.8563\n",
            "Epoch 53/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.5953 - accuracy: 0.8487 - val_loss: 0.5777 - val_accuracy: 0.8597\n",
            "Epoch 54/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.5950 - accuracy: 0.8481 - val_loss: 0.5472 - val_accuracy: 0.8690\n",
            "Epoch 55/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5910 - accuracy: 0.8493 - val_loss: 0.6552 - val_accuracy: 0.8366\n",
            "Epoch 56/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5901 - accuracy: 0.8505 - val_loss: 0.5780 - val_accuracy: 0.8615\n",
            "Epoch 57/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5890 - accuracy: 0.8501 - val_loss: 0.5667 - val_accuracy: 0.8630\n",
            "Epoch 58/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5871 - accuracy: 0.8511 - val_loss: 0.6087 - val_accuracy: 0.8501\n",
            "Epoch 59/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5831 - accuracy: 0.8536 - val_loss: 0.5911 - val_accuracy: 0.8556\n",
            "Epoch 60/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5914 - accuracy: 0.8501 - val_loss: 0.5944 - val_accuracy: 0.8559\n",
            "Epoch 61/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5889 - accuracy: 0.8512 - val_loss: 0.5948 - val_accuracy: 0.8537\n",
            "Epoch 62/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5880 - accuracy: 0.8506 - val_loss: 0.6162 - val_accuracy: 0.8463\n",
            "Epoch 63/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5861 - accuracy: 0.8508 - val_loss: 0.6372 - val_accuracy: 0.8433\n",
            "Epoch 64/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5856 - accuracy: 0.8513 - val_loss: 0.6351 - val_accuracy: 0.8439\n",
            "Epoch 65/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5824 - accuracy: 0.8515 - val_loss: 0.6515 - val_accuracy: 0.8414\n",
            "Epoch 66/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5821 - accuracy: 0.8518 - val_loss: 0.5370 - val_accuracy: 0.8735\n",
            "Epoch 67/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5841 - accuracy: 0.8534 - val_loss: 0.6333 - val_accuracy: 0.8470\n",
            "Epoch 68/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5786 - accuracy: 0.8547 - val_loss: 0.5401 - val_accuracy: 0.8707\n",
            "Epoch 69/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5816 - accuracy: 0.8522 - val_loss: 0.5480 - val_accuracy: 0.8710\n",
            "Epoch 70/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5805 - accuracy: 0.8550 - val_loss: 0.5832 - val_accuracy: 0.8594\n",
            "Epoch 71/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5802 - accuracy: 0.8546 - val_loss: 0.5872 - val_accuracy: 0.8565\n",
            "Epoch 72/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.5833 - accuracy: 0.8519 - val_loss: 0.5733 - val_accuracy: 0.8599\n",
            "Epoch 73/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5781 - accuracy: 0.8545 - val_loss: 0.6293 - val_accuracy: 0.8459\n",
            "Epoch 74/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5787 - accuracy: 0.8547 - val_loss: 0.5699 - val_accuracy: 0.8610\n",
            "Epoch 75/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.5740 - accuracy: 0.8564 - val_loss: 0.6759 - val_accuracy: 0.8336\n",
            "Epoch 76/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5766 - accuracy: 0.8539 - val_loss: 0.5951 - val_accuracy: 0.8588\n",
            "Epoch 77/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5339 - accuracy: 0.8693 - val_loss: 0.5427 - val_accuracy: 0.8708\n",
            "Epoch 78/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5121 - accuracy: 0.8747 - val_loss: 0.5484 - val_accuracy: 0.8661\n",
            "Epoch 79/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5103 - accuracy: 0.8737 - val_loss: 0.5159 - val_accuracy: 0.8766\n",
            "Epoch 80/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5003 - accuracy: 0.8766 - val_loss: 0.4952 - val_accuracy: 0.8848\n",
            "Epoch 81/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4970 - accuracy: 0.8776 - val_loss: 0.5480 - val_accuracy: 0.8658\n",
            "Epoch 82/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4907 - accuracy: 0.8788 - val_loss: 0.5121 - val_accuracy: 0.8781\n",
            "Epoch 83/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4906 - accuracy: 0.8773 - val_loss: 0.4912 - val_accuracy: 0.8822\n",
            "Epoch 84/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4789 - accuracy: 0.8798 - val_loss: 0.5317 - val_accuracy: 0.8716\n",
            "Epoch 85/125\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 0.4806 - accuracy: 0.8800 - val_loss: 0.5112 - val_accuracy: 0.8764\n",
            "Epoch 86/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4806 - accuracy: 0.8797 - val_loss: 0.4968 - val_accuracy: 0.8782\n",
            "Epoch 87/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4748 - accuracy: 0.8820 - val_loss: 0.5102 - val_accuracy: 0.8729\n",
            "Epoch 88/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4776 - accuracy: 0.8794 - val_loss: 0.5200 - val_accuracy: 0.8746\n",
            "Epoch 89/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4718 - accuracy: 0.8810 - val_loss: 0.4928 - val_accuracy: 0.8779\n",
            "Epoch 90/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4691 - accuracy: 0.8809 - val_loss: 0.5284 - val_accuracy: 0.8711\n",
            "Epoch 91/125\n",
            "781/781 [==============================] - 25s 31ms/step - loss: 0.4691 - accuracy: 0.8813 - val_loss: 0.5021 - val_accuracy: 0.8742\n",
            "Epoch 92/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4687 - accuracy: 0.8805 - val_loss: 0.5000 - val_accuracy: 0.8769\n",
            "Epoch 93/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4639 - accuracy: 0.8829 - val_loss: 0.5397 - val_accuracy: 0.8632\n",
            "Epoch 94/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4670 - accuracy: 0.8816 - val_loss: 0.4882 - val_accuracy: 0.8755\n",
            "Epoch 95/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4586 - accuracy: 0.8821 - val_loss: 0.5129 - val_accuracy: 0.8742\n",
            "Epoch 96/125\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4614 - accuracy: 0.8831 - val_loss: 0.5009 - val_accuracy: 0.8720\n",
            "Epoch 97/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4593 - accuracy: 0.8810 - val_loss: 0.5054 - val_accuracy: 0.8728\n",
            "Epoch 98/125\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4604 - accuracy: 0.8825 - val_loss: 0.5035 - val_accuracy: 0.8779\n",
            "Epoch 99/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4570 - accuracy: 0.8832 - val_loss: 0.4989 - val_accuracy: 0.8750\n",
            "Epoch 100/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4550 - accuracy: 0.8822 - val_loss: 0.4744 - val_accuracy: 0.8853\n",
            "Epoch 101/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4558 - accuracy: 0.8817 - val_loss: 0.5047 - val_accuracy: 0.8745\n",
            "Epoch 102/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4313 - accuracy: 0.8926 - val_loss: 0.4955 - val_accuracy: 0.8779\n",
            "Epoch 103/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4296 - accuracy: 0.8901 - val_loss: 0.4695 - val_accuracy: 0.8815\n",
            "Epoch 104/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4212 - accuracy: 0.8934 - val_loss: 0.4694 - val_accuracy: 0.8854\n",
            "Epoch 105/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4169 - accuracy: 0.8949 - val_loss: 0.4871 - val_accuracy: 0.8784\n",
            "Epoch 106/125\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 0.4190 - accuracy: 0.8947 - val_loss: 0.4632 - val_accuracy: 0.8854\n",
            "Epoch 107/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4135 - accuracy: 0.8950 - val_loss: 0.4600 - val_accuracy: 0.8836\n",
            "Epoch 108/125\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 0.4129 - accuracy: 0.8943 - val_loss: 0.4532 - val_accuracy: 0.8883\n",
            "Epoch 109/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4125 - accuracy: 0.8957 - val_loss: 0.4537 - val_accuracy: 0.8886\n",
            "Epoch 110/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4132 - accuracy: 0.8947 - val_loss: 0.4589 - val_accuracy: 0.8853\n",
            "Epoch 111/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4080 - accuracy: 0.8966 - val_loss: 0.4423 - val_accuracy: 0.8912\n",
            "Epoch 112/125\n",
            "781/781 [==============================] - 31s 39ms/step - loss: 0.4058 - accuracy: 0.8966 - val_loss: 0.4765 - val_accuracy: 0.8805\n",
            "Epoch 113/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.4081 - accuracy: 0.8958 - val_loss: 0.4715 - val_accuracy: 0.8822\n",
            "Epoch 114/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.4068 - accuracy: 0.8959 - val_loss: 0.4774 - val_accuracy: 0.8783\n",
            "Epoch 115/125\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 0.4017 - accuracy: 0.8965 - val_loss: 0.4410 - val_accuracy: 0.8883\n",
            "Epoch 116/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4022 - accuracy: 0.8962 - val_loss: 0.4477 - val_accuracy: 0.8876\n",
            "Epoch 117/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4008 - accuracy: 0.8969 - val_loss: 0.4328 - val_accuracy: 0.8928\n",
            "Epoch 118/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4018 - accuracy: 0.8957 - val_loss: 0.4655 - val_accuracy: 0.8850\n",
            "Epoch 119/125\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.3992 - accuracy: 0.8985 - val_loss: 0.4648 - val_accuracy: 0.8836\n",
            "Epoch 120/125\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 0.4027 - accuracy: 0.8950 - val_loss: 0.4935 - val_accuracy: 0.8739\n",
            "Epoch 121/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.3983 - accuracy: 0.8976 - val_loss: 0.4915 - val_accuracy: 0.8737\n",
            "Epoch 122/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4010 - accuracy: 0.8964 - val_loss: 0.4803 - val_accuracy: 0.8765\n",
            "Epoch 123/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.3998 - accuracy: 0.8973 - val_loss: 0.4448 - val_accuracy: 0.8863\n",
            "Epoch 124/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.3937 - accuracy: 0.8979 - val_loss: 0.4448 - val_accuracy: 0.8879\n",
            "Epoch 125/125\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.3964 - accuracy: 0.8947 - val_loss: 0.4860 - val_accuracy: 0.8784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD5O31vBxLfC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-z5gTs_FNs6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}